## ElasticSearch 分词

一个分词器 tokenizer 将字符流分割成独立的词元 token ,一个token通常是一个独立的单词，然后输出 token流



### 1. 标准分词器

es默认使用的是 standard 分词器

 效果如下：



```json
post _analyze
{
  "analyzer":"standard",
  "text":"中文分词器"
}
```

```json
{
  "tokens": [
    {
      "token": "中",
      "start_offset": 0,
      "end_offset": 1,
      "type": "<IDEOGRAPHIC>",
      "position": 0
    },
    {
      "token": "文",
      "start_offset": 1,
      "end_offset": 2,
      "type": "<IDEOGRAPHIC>",
      "position": 1
    },
    {
      "token": "分",
      "start_offset": 2,
      "end_offset": 3,
      "type": "<IDEOGRAPHIC>",
      "position": 2
    },
    {
      "token": "词",
      "start_offset": 3,
      "end_offset": 4,
      "type": "<IDEOGRAPHIC>",
      "position": 3
    },
    {
      "token": "器",
      "start_offset": 4,
      "end_offset": 5,
      "type": "<IDEOGRAPHIC>",
      "position": 4
    }
  ]
}
```

对于中文的分词效果不好，会将每个汉字单独分词。因此需要使用插件引入中文的分词器



### 2. 中文分词器

常用的是ik分词器

https://github.com/medcl/elasticsearch-analysis-ik



#### 2.1 安装

1.下载安装

下载es对应版本的分词器，如：

https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v8.7.0/elasticsearch-analysis-ik-8.7.0.zip

在es-root/plugins/ 目录下新建文件夹 ik 然后将下载的zip包解压至该文件夹下即可

2.命令安装

```bash
./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v8.7.0/elasticsearch-analysis-ik-8.7.0.zip
```



检查插件：

```bash
./bin/elasticsearch-plugin list
```

输出有 ik 即表明已经安装好了， 安装插件之后需要重启es才能使用新安装的分词器。



#### 2.2 测试

ik分词器内置2个analyzer， ik_smart 和 ik_max

分别测试如下：

- ik_smart

会做最粗粒度的拆分

```json
post _analyze
{
  "analyzer":"ik_smart",
  "text":"中文分词器"
}
```

结果如下：

```json
{
  "tokens": [
    {
      "token": "中文",
      "start_offset": 0,
      "end_offset": 2,
      "type": "CN_WORD",
      "position": 0
    },
    {
      "token": "分词器",
      "start_offset": 2,
      "end_offset": 5,
      "type": "CN_WORD",
      "position": 1
    }
  ]
}
```



- ik_max_word

会做最细粒度的拆分

```json
post _analyze
{
  "analyzer":"ik_max_word",
  "text":"中文分词器"
}
```

结果如下：

```json
{
  "tokens": [
    {
      "token": "中文",
      "start_offset": 0,
      "end_offset": 2,
      "type": "CN_WORD",
      "position": 0
    },
    {
      "token": "分词器",
      "start_offset": 2,
      "end_offset": 5,
      "type": "CN_WORD",
      "position": 1
    },
    {
      "token": "分词",
      "start_offset": 2,
      "end_offset": 4,
      "type": "CN_WORD",
      "position": 2
    },
    {
      "token": "器",
      "start_offset": 4,
      "end_offset": 5,
      "type": "CN_CHAR",
      "position": 3
    }
  ]
}
```

两种分词器使用的最佳实践是：索引时用ik_max_word，在搜索时用ik_smart。即：索引时最大化的将文章内容分词，搜索时更精确的搜索到想要的结果。



#### 2.3 自定义词典

1 创建nginx服务，在nginx静态资源目录下创建 es/ik 目录，在目录下创建  ext_dict.txt 和 ext_stopwords.txt 用于存储自定义词典和自定义停止词，确保下列接口可以正确返回

```http
http://localhost/es/ik/ext_dict.txt
```

```http
http://localhost/es/ik/ext_stopwords.txt
```

在 ext_dict中加入词汇，一个词汇单独占一行，如：

```
电商
```

在ext_stopwords中添加停止词

```
的
啊
呢
吧
```

2 打开 \elasticsearch\plugins\ik\config目录，修改 IKAnalyzer.cfg.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">
<properties>
	<comment>IK Analyzer 扩展配置</comment>
	<!--用户可以在这里配置自己的扩展字典 -->
	<entry key="ext_dict"></entry>
	 <!--用户可以在这里配置自己的扩展停止词字典-->
	<entry key="ext_stopwords"></entry>
	<!--用户可以在这里配置远程扩展字典 -->
	<entry key="remote_ext_dict">http://localhost:80/es/ik/ext_dict.txt</entry>
	<!--用户可以在这里配置远程扩展停止词字典-->
	<entry key="remote_ext_stopwords">http://localhost:80/es/ik/ext_stopwords.txt</entry>
</properties>
```

使用nginx下的词典作为扩展词典



3 重启 es ，测试分词效果

```json
post _analyze
{
  "analyzer":"ik_smart",
  "text":"让我们来开发一个电商项目吧"
}
```

返回结果：

```json
{
  "tokens": [
    {
      "token": "让我们",
      "start_offset": 0,
      "end_offset": 3,
      "type": "CN_WORD",
      "position": 0
    },
    {
      "token": "来",
      "start_offset": 3,
      "end_offset": 4,
      "type": "CN_CHAR",
      "position": 1
    },
    {
      "token": "开发",
      "start_offset": 4,
      "end_offset": 6,
      "type": "CN_WORD",
      "position": 2
    },
    {
      "token": "一个",
      "start_offset": 6,
      "end_offset": 8,
      "type": "CN_WORD",
      "position": 3
    },
    {
      "token": "电商",
      "start_offset": 8,
      "end_offset": 10,
      "type": "CN_WORD",
      "position": 4
    },
    {
      "token": "项目",
      "start_offset": 10,
      "end_offset": 12,
      "type": "CN_WORD",
      "position": 5
    }
  ]
}
```

注意可以识别词汇 “电商”，并且删除了 “吧”

