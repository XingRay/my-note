## k8s二进制高可用集群

## 1 资源规划

节点规划

这里使用虚拟机，网络模式使用桥接模式

| 名称             | 节点角色      | 公网ip        | 内网ip        | 账号          | cpu  | 内存(G) | 磁盘(G) |
| ---------------- | ------------- | ------------- | ------------- | ------------- | ---- | ------- | ------- |
| k8s-master01     | master / etcd | 192.168.0.151 | 192.168.0.151 | root/123456   | 2    | 4       | 32      |
| k8s-master02     | master / etcd | 192.168.0.152 | 192.168.0.152 | root/123456   | 2    | 4       | 32      |
| k8s-master03     | master / etcd | 192.168.0.153 | 192.168.0.153 | root/123456   | 2    | 4       | 32      |
| k8s-worker01     | worker        | 192.168.0.161 | 192.168.0.161 | root/123456   | 8    | 16      | 32      |
| k8s-worker02     | worker        | 192.168.0.162 | 192.168.0.162 | root/123456   | 8    | 16      | 32      |
| k8s-worker03     | worker        | 192.168.0.163 | 192.168.0.163 | root/123456   | 8    | 16      | 32      |
| k8s-loadbalancer | loadbalancer  | 192.168.0.250 | 192.168.0.250 | root / 123456 | 2    | 4       | 20      |



192.168.0.x     : 为节点的网段

10.96.0.0/16   : 为Service网段

196.16.0.0/16 : 为Pod网段





## 2 安装系统



![image-20230820155707200](assets/008_k8s_二进制高可用集群/image-20230820155707200.png)



![image-20230820155748180](assets/008_k8s_二进制高可用集群/image-20230820155748180.png)



![image-20230820155804939](assets/008_k8s_二进制高可用集群/image-20230820155804939.png)



![image-20230820155839474](assets/008_k8s_二进制高可用集群/image-20230820155839474.png)



开始安装系统

![image-20230820155925910](assets/008_k8s_二进制高可用集群/image-20230820155925910.png)



![image-20230820160152102](assets/008_k8s_二进制高可用集群/image-20230820160152102.png)

![image-20230820160242220](assets/008_k8s_二进制高可用集群/image-20230820160242220.png)

![image-20230820160319845](assets/008_k8s_二进制高可用集群/image-20230820160319845.png)

![image-20230820160407049](assets/008_k8s_二进制高可用集群/image-20230820160407049.png)



![image-20230820160424392](assets/008_k8s_二进制高可用集群/image-20230820160424392.png)



![image-20230820160449450](assets/008_k8s_二进制高可用集群/image-20230820160449450.png)



![image-20230820161111602](assets/008_k8s_二进制高可用集群/image-20230820161111602.png)



https://mirrors.aliyun.com/ubuntu/

![image-20230820161322013](assets/008_k8s_二进制高可用集群/image-20230820161322013.png)

![image-20230820161504856](assets/008_k8s_二进制高可用集群/image-20230820161504856.png)



![image-20230820161526357](assets/008_k8s_二进制高可用集群/image-20230820161526357.png)



![image-20230820161541205](assets/008_k8s_二进制高可用集群/image-20230820161541205.png)



![image-20230820161603335](assets/008_k8s_二进制高可用集群/image-20230820161603335.png)



![image-20230820161614124](assets/008_k8s_二进制高可用集群/image-20230820161614124.png)



tom / 123456

服务器名根据规划设置 : k8s-master01

![image-20230820161705969](assets/008_k8s_二进制高可用集群/image-20230820161705969.png)



![image-20230820161804798](assets/008_k8s_二进制高可用集群/image-20230820161804798.png)



![image-20230820161816603](assets/008_k8s_二进制高可用集群/image-20230820161816603.png)



![image-20230820161827884](assets/008_k8s_二进制高可用集群/image-20230820161827884.png)



![image-20230820161932005](assets/008_k8s_二进制高可用集群/image-20230820161932005.png)





![image-20230820161946379](assets/008_k8s_二进制高可用集群/image-20230820161946379.png)



![image-20230820170520398](assets/008_k8s_二进制高可用集群/image-20230820170520398.png)





![image-20230820170545702](assets/008_k8s_二进制高可用集群/image-20230820170545702.png)



![image-20230820171154301](assets/008_k8s_二进制高可用集群/image-20230820171154301.png)





![image-20230820171344529](assets/008_k8s_二进制高可用集群/image-20230820171344529.png)



做系统快照 1 : os_installed



## 3 配置root远程登录

登录 tom / 123456



设置root密码

```bash
sudo passwd root
```

第一次输入当前账号的密码

然后输入两次为root账号设置的密码 123456

```bash
exit
```

用root账号登录

root / 123456



设置ssh

```bash
apt install -y openssh-server
```

修改允许远程登录：

将 /etc/ssh/sshd_config 中的配置行 PermitRootLogin prohibit-password 修改为  PermitRootLogin yes

```shell
sudo vi /etc/ssh/sshd_config
```

或者

```shell
sudo sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config
```

重启ssh:

```bash
sudo systemctl restart ssh
```



通过 ssh工具连接ubuntu



做系统快照 2 : ssh



使用shell工具连接所有节点

![image-20230821190345540](assets/008_k8s_二进制高可用集群/image-20230821190345540.png)



## 4 配置 host 信息

所有节点 查看 hostname 

```shell
hostname
```

如果由于某些原因在系统安装是没有设置好指定的hostname, 可以通过下面的指令修改, 例如将设置第一个主节点的 hostname, 其他所有节点类似,根据节点规划设置 hostname :

```shell
sudo hostnamectl set-hostname k8s-master01
```

编辑hosts文件

```shell
 vi /etc/hosts
```

注意如果在上面修改了hostname, 在这里也需要做更新 127.0.0.1 对应的域名为更新后的 hostname

```shell
127.0.0.1 localhost
127.0.1.1 k8s-master01

# The following lines are desirable for IPv6 capable hosts
::1     ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
```

所有节点都在 hosts文件中的最后添加下面的内容

```shell
192.168.0.151 k8s-master01
192.168.0.152 k8s-master02
192.168.0.153 k8s-master03
192.168.0.161 k8s-worker01
192.168.0.162 k8s-worker02
192.168.0.163 k8s-worker03
192.168.0.250 k8s-loadbalancer
```

所有节点都执行测试:

```shell
ping -c 4 k8s-master01
ping -c 4 k8s-master02
ping -c 4 k8s-master03
ping -c 4 k8s-worker01
ping -c 4 k8s-worker02
ping -c 4 k8s-worker03
ping -c 4 k8s-loadbalancer
```

确保所有节点都相互可以ping通



## 5 SSH远程免密登录


集群的各个服务器之间，可以相互信任，设置免密登录

1 生成密钥对

在所有节点执行

```shell
ssh-keygen -t rsa
```

按照默认设置, 输入三次enter即可生成密钥对



2 授权
授权的过程其实就是将A服务器的公钥分别给自己、B、C都拷贝一份。使用命令

```
ssh-copy-id k8s-master01
ssh-copy-id k8s-master02
ssh-copy-id k8s-master03
ssh-copy-id k8s-worker01
ssh-copy-id k8s-worker02
ssh-copy-id k8s-worker03
ssh-copy-id k8s-loadbalancer
```



```bash
ssh-copy-id 192.168.0.151
ssh-copy-id 192.168.0.152
ssh-copy-id 192.168.0.153
ssh-copy-id 192.168.0.161
ssh-copy-id 192.168.0.162
ssh-copy-id 192.168.0.163
ssh-copy-id 192.168.0.250
```



命令执行期间根据提示输入目标节点的 root 密码 和 yes



测试, 所有节点依次执行下列命令

```bash
ssh root@k8s-master01
ssh root@k8s-master02
ssh root@k8s-master03
ssh root@k8s-worker01
ssh root@k8s-worker02
ssh root@k8s-worker03
ssh root@k8s-loadbalancer

ssh root@192.168.0.151
ssh root@192.168.0.152
ssh root@192.168.0.153
ssh root@192.168.0.161
ssh root@192.168.0.162
ssh root@192.168.0.163
ssh root@192.168.0.250
```

每次正确进入目标节点后执行

```shell
exit
```

退出即可



## 7 安装cfssl

CFSSL是CloudFlare开源的一款PKI/TLS工具。 CFSSL 包含一个命令行工具 和一个用于 签名，验证并且捆绑TLS证书的 HTTP API 服务。 使用Go语言编写。

Github 地址： https://github.com/cloudflare/cfssl



安装：去官网下载  `cfssl-certinfo_linux-amd64` `cfssljson_linux-amd64` `cfssl_linux-amd64` 这三个组件

目前最新版本:

https://github.com/cloudflare/cfssl/releases/download/v1.6.4/cfssl_1.6.4_linux_amd64

https://github.com/cloudflare/cfssl/releases/download/v1.6.4/cfssl-certinfo_1.6.4_linux_amd64

https://github.com/cloudflare/cfssl/releases/download/v1.6.4/cfssljson_1.6.4_linux_amd64

下载后上传至服务器 `k8s_install/cfssl` 目录



以下是一个 shell 脚本的示例，它会将当前目录下 cfssl 子目录中的文件到 /usr/bin 目录，并为这些文件添加执行权限：你可以通过使用 ls 命令来获取当前 cfssl 目录中的文件列表，然后动态地设置 files 数组。以下是相应的修改脚本：

```shell
vi cfssl_install.sh
```



```shell
#!/bin/bash

# 获取当前 cfssl 目录中的文件列表
files=$(ls cfssl)

# 循环遍历所有节点，并进行文件复制和权限设置

for file in $files; do
	cp "cfssl/$file" "/usr/bin/$file"
	chmod +x "/usr/bin/$file"
	echo "Copied and chmod +x $file."
done


echo "File copying and permission setting complete."
```

这个脚本中，我们使用了 ls cfssl 来获取当前 cfssl 目录中的文件列表，并将其赋值给 files 变量。然后脚本会循环遍历所有节点和文件，使用 scp 命令将每个文件复制到目标节点的 /usr/bin 目录，并使用 SSH 命令为每个文件添加执行权限。

将以上内容保存为一个文件（比如 cfssl_install.sh），然后给脚本执行权限：

    chmod +x cfssl_install.sh

然后你就可以在任意节点上执行这个脚本：

    ./cfssl_install.sh

这个脚本会动态地获取 cfssl 目录中的文件列表，并将它们复制到所有节点的 /usr/bin 目录下，并为这些文件添加执行权限。



卸载

当你想要删除之前复制的文件，可以编写一个简单的 shell 脚本来卸载这些文件。以下是一个示例脚本，它会删除 `/usr/bin` 目录下的特定文件：

```shell
#!/bin/bash

# 列出要删除的文件列表
files=(
  "cfssl"
  "cfssl-certinfo"
  "cfssljson"
)

# 循环遍历文件列表并删除文件
for file in "${files[@]}"; do
  rm "/usr/bin/$file"
  echo "Removed $file from /usr/bin/"
done

echo "File removal complete."
```

将以上内容保存为一个文件（比如 `uninstall.sh`），然后给脚本执行权限：

```shell
chmod +x cfssl_uninstall.sh
```

然后你就可以在终端中执行这个脚本：

```shell
./cfssl_uninstall.sh
```

这个脚本会遍历指定的文件列表，并从 `/usr/bin` 目录中删除这些文件。

请确保在执行卸载脚本之前备份重要文件，以免误删除。



## 8 生成证书

创建一个ssl目录

```shell
mkdir ssl
cd ssl
```



再创建k8s目录

```shell
mkdir k8s
cd k8s
```

创建配置文件

```shell
vi ca-config.json
```

```json
{
	"signing":{
		"default":{
			"expiry":"87600h"
		},
		"profiles":{
			"kubernetes":{
				"usages":[
					"signing",
					"key encipherment",
					"server auth",
					"client auth"
				],
				"expiry":"87600h"
			}
		}
	}
}
```

常见ca申请书:

```shell
vi ca-csr.json
```

```json
{
  "CN": "kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "BeiJing",
      "O": "kubernetes",
      "ST": "BeiJing",
      "OU": "kubernetes"
    }
  ]
}
```

生成ca证书

```shell
cfssl gencert -initca ca-csr.json | cfssljson -bare ca
```

```shell
root@k8s-master01:~/k8s_install/ssl/k8s# cfssl gencert -initca ca-csr.json | cfssljson -bare ca
2023/08/21 13:38:33 [INFO] generating a new CA key and certificate from CSR
2023/08/21 13:38:33 [INFO] generate received request
2023/08/21 13:38:33 [INFO] received CSR
2023/08/21 13:38:33 [INFO] generating key: rsa-2048
2023/08/21 13:38:33 [INFO] encoded CSR
2023/08/21 13:38:33 [INFO] signed certificate with serial number 638546255871888892923988801920906380534866016781
root@k8s-master01:~/k8s_install/ssl/k8s# ls
ca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem
```

生成CA所必需的文件ca-key.pem（私钥）和ca.pem（证书），还会生成ca.csr（证书签名请求），用于交叉签名或重新签名。  



生成ca配置

client certificate： 用于服务端认证客户端,例如etcdctl、etcd proxy、fleetctl、docker 客户端

server certificate: 服务端使用，客户端以此验证服务端身份,例如docker服务端、kube-apiserver

peer certificate: 双向证书，用于 etcd 集群成员间通信



创建ca配置文件  ca-config.json

相当于证书颁发机构的工作规章制度

"ca-config.json"：可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个 profile

"signing"：表示该证书可用于签名其它证书；生成的 ca.pem 证书中 CA=TRUE

"server auth"：表示client可以用该 CA 对server提供的证书进行验证

 "client auth"：表示server可以用该CA对client提供的证书进行验证

```json
{
  "signing": {
    "default": {
      "expiry": "43800h"
    },
    "profiles": {
      "server": {
        "expiry": "43800h",
        "usages": [
          "signing",
          "key encipherment",
          "server auth"
        ]
      },
      "client": {
        "expiry": "43800h",
        "usages": [
          "signing",
          "key encipherment",
          "client auth"
        ]
      },
      "peer": {
        "expiry": "43800h",
        "usages": [
          "signing",
          "key encipherment",
          "server auth",
          "client auth"
        ]
      },
      "kubernetes": {
        "expiry": "43800h",
        "usages": [
          "signing",
          "key encipherment",
          "server auth",
          "client auth"
        ]
      },
      "etcd": {
        "expiry": "43800h",
        "usages": [
          "signing",
          "key encipherment",
          "server auth",
          "client auth"
        ]
      }
    }
  }
}
```



准备一个证书申请请求书 `csr.json` 。证书机构就会根据我们请求签发证书,  使用命令打印模板

```
cfssl print-defaults csr
```

```json
{
    "CN": "example.net",
    "hosts": [
        "example.net",
        "www.example.net"
    ],
    "key": {
        "algo": "ecdsa",
        "size": 256
    },
    "names": [
        {
            "C": "US",
            "ST": "CA",
            "L": "San Francisco"
        }
    ]
}
```

"CN": "example.net", 浏览器验证该字段是否合法，一般写域名，非常重要.  

创建ca证书签名 ca-csr.json

"CN"：Common Name，从证书中提取该字段作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法； 

"O"：Organization，从证书中提取该字段作为请求用户所属的组 (Group)；

这两个参数在后面的 kubernetes 启用 RBAC 模式中很重要，因为需要设置kubelet、admin 等角色权限，那么在配置证书的时候就必须配置对了，具体后面在部署 kubernetes 的时候会进行讲解。在etcd这两个参数没太大的重要意义，跟着配置就好。

```shell
vi ca-csr.json
```

```json
{
  "CN": "SelfSignedCa",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "beijing",
      "O": "cfssl",
      "ST": "beijing",
      "OU": "System"
    }
  ]
}
```



生成ca证书和私钥  

```shell
cfssl gencert -initca ca-csr.json | cfssljson -bare ca -
```

```shell
root@k8s-master01:~/k8s_install/ssl/k8s# cfssl gencert -initca ca-csr.json | cfssljson -bare ca -
2023/08/21 14:11:01 [INFO] generating a new CA key and certificate from CSR
2023/08/21 14:11:01 [INFO] generate received request
2023/08/21 14:11:01 [INFO] received CSR
2023/08/21 14:11:01 [INFO] generating key: rsa-2048
2023/08/21 14:11:01 [INFO] encoded CSR
2023/08/21 14:11:01 [INFO] signed certificate with serial number 493471227768287772316098840576288191284241835548
root@k8s-master01:~/k8s_install/ssl/k8s# ls
ca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem
```

ca.csr   证书签名请求, 用于交叉签名或重新签名。  

ca.pem  ca公钥

ca-key.pem  ca私钥,妥善保管



创建etcd证书签名 etcd-csr.json

```shell
vi etcd-csr.json
```

```json
{
  "CN": "etcd",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "shanghai",
      "O": "etcd",
      "ST": "shanghai",
      "OU": "System"
    }
  ]
}
```



生成etcd证书  

```shell
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=etcd etcd-csr.json | cfssljson -bare etcd
```

```shell
root@k8s-master01:~/k8s_install/ssl/k8s# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=etcd etcd-csr.json | cfssljson -bare etcd
2023/08/21 14:16:18 [INFO] generate received request
2023/08/21 14:16:18 [INFO] received CSR
2023/08/21 14:16:18 [INFO] generating key: rsa-2048
2023/08/21 14:16:18 [INFO] encoded CSR
2023/08/21 14:16:18 [INFO] signed certificate with serial number 504898311543665953941966912219142147336727229124
2023/08/21 14:16:18 [WARNING] This certificate lacks a "hosts" field. This makes it unsuitable for
websites. For more information see the Baseline Requirements for the Issuance and Management
of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 ("Information Requirements").
root@k8s-master01:~/k8s_install/ssl/k8s# ls
ca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem  etcd.csr  etcd-csr.json  etcd-key.pem  etcd.pem
```



创建kubernetes证书签名(kubernetes-csr.json)  

```json
{
  "CN": "kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "shanghai",
      "O": "kubernetes",
      "ST": "shanghai",
      "OU": "System"
    }
  ]
}
```

生成k8s证书

```shell
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes
```

```shell
root@k8s-master01:~/k8s_install/ssl/k8s# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes
2023/08/21 14:19:20 [INFO] generate received request
2023/08/21 14:19:20 [INFO] received CSR
2023/08/21 14:19:20 [INFO] generating key: rsa-2048
2023/08/21 14:19:20 [INFO] encoded CSR
2023/08/21 14:19:20 [INFO] signed certificate with serial number 453460888699692490471303734356758149505949678045
2023/08/21 14:19:20 [WARNING] This certificate lacks a "hosts" field. This makes it unsuitable for
websites. For more information see the Baseline Requirements for the Issuance and Management
of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 ("Information Requirements").
root@k8s-master01:~/k8s_install/ssl/k8s# ls
ca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem  etcd.csr  etcd-csr.json  etcd-key.pem  etcd.pem  kubernetes.csr  kubernetes-csr.json  kubernetes-key.pem  kubernetes.pem
```

kubernetes.csr 

kubernetes-key.pem 

kubernetes.pem



最后校验证书是否合适  

```shell
openssl x509 -in ca.pem -text -noout
openssl x509 -in etcd.pem -text -noout
openssl x509 -in kubernetes.pem -text -noout
```



## 9 集群搭建

所有节点基础环境

192.168.0.x     : 为节点的网段

10.96.0.0/16   : 为Service网段

196.16.0.0/16 : 为Pod网段



### 1 安装工具

每个节点执行:

```shell
apt install -y selinux-utils vim wget net-tools git jq psmisc lvm2 ipvsadm ipset sysstat conntrack curl gnupg2 software-properties-common apt-transport-https ca-certificates
```



### 2 设置时区

```shell
timedatectl set-timezone Asia/Shanghai
```



### 3 关闭selinux

```shell
apt install -y selinux-utils
setenforce 0
```

/etc/selinux/config  添加一行   SELINUX=disabled  

```shell
echo SELINUX=disabled >> /etc/selinux/config
```



### 4 关闭swap

```shell
swapoff -a && sysctl -w vm.swappiness=0
sed -i '/^\/swap.img/ s/^/#/' /etc/fstab
```



### 5 修改资源限制

```
ulimit -SHn 65535
```

修改 /etc/security/limits.conf

```
vi /etc/security/limits.conf
```

添加:

```shell
* soft nofile 655360
* hard nofile 131072
* soft nproc 655350
* hard nproc 655350
* soft memlock unlimited
* hard memlock unlimited
```



修改 /etc/sysctl.conf

```shell
vi /etc/sysctl.conf
```

添加:

```shell
fs.inotify.max_queued_events = 32768
fs.inotify.max_user_instances = 65536
fs.inotify.max_user_watches = 1048576
```

修改生效:

```
sysctl -p
```



### 6 配置 ipvs 模块

```shell
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack
```

在 /etc/modules-load.d/ipvs.conf 中添加内容:

```shell
vi /etc/modules-load.d/ipvs.conf
```



```shell
ip_vs
ip_vs_lc
ip_vs_wlc
ip_vs_rr
ip_vs_wrr
ip_vs_lblc
ip_vs_lblcr
ip_vs_dh
ip_vs_sh
ip_vs_fo
ip_vs_nq
ip_vs_sed
ip_vs_ftp
ip_vs_sh
nf_conntrack
ip_tables
ip_set
xt_set
ipt_set
ipt_rpfilter
ipt_REJECT
ipip
```

启动模块

```
systemctl enable --now systemd-modules-load.service
```



### 7 为 containerd 加载内核模块

```shell
vi /etc/modules-load.d/containerd.conf
```

添加内容:

```shell
overlay
br_netfilter
```

执行:

```shell
modprobe overlay
modprobe br_netfilter
```



### 8 为 Kubernetes 设置内核参数

```shell
vi /etc/sysctl.d/kubernetes.conf
```

添加内容:

```shell
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
fs.may_detach_mounts = 1
vm.overcommit_memory=1
net.ipv4.conf.all.route_localnet = 1
vm.panic_on_oom=0
fs.inotify.max_user_watches=89100
fs.file-max=52706963
fs.nr_open=52706963
net.netfilter.nf_conntrack_max=2310720
net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_probes = 3
net.ipv4.tcp_keepalive_intvl =15
net.ipv4.tcp_max_tw_buckets = 36000
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_max_orphans = 327680
net.ipv4.tcp_orphan_retries = 3
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_max_syn_backlog = 16768
net.ipv4.ip_conntrack_max = 65536
net.ipv4.tcp_timestamps = 0
net.core.somaxconn = 16768
```

修改生效:

```shell
sysctl --system
```



### 9 安装containerd

https://containerd.io/downloads/

https://github.com/containerd/containerd/blob/main/docs/getting-started.md



下载 

https://github.com/containerd/containerd

最新版本为:

https://github.com/containerd/containerd/releases/tag/v1.7.3

下载:

https://github.com/containerd/containerd/releases/download/v1.7.3/containerd-1.7.3-linux-amd64.tar.gz

上传至服务器



可以上传到单个服务器后进行分发

```shell
# 从本机复制到 k8s-master02
scp -r ~/k8s_install/containerd/* k8s-master02:~/k8s_install/containerd/

# 从本机复制到 k8s-master03
scp -r ~/k8s_install/containerd/* k8s-master03:~/k8s_install/containerd/

# 从本机复制到 k8s-worker01
scp -r ~/k8s_install/containerd/* k8s-worker01:~/k8s_install/containerd/

# 从本机复制到 k8s-worker02
scp -r ~/k8s_install/containerd/* k8s-worker02:~/k8s_install/containerd/

# 从本机复制到 k8s-worker03
scp -r ~/k8s_install/containerd/* k8s-worker03:~/k8s_install/containerd/
```



解压:

```shell
tar -zxvf containerd-1.7.3-linux-amd64.tar.gz -C /usr/local
```

创建service

```shell
vi containerd.service
```

输入:

```toml
# Copyright The containerd Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

[Unit]
Description=containerd container runtime
Documentation=https://containerd.io
After=network.target local-fs.target

[Service]
#uncomment to enable the experimental sbservice (sandboxed) version of containerd/cri integration
#Environment="ENABLE_CRI_SANDBOXES=sandboxed"
ExecStartPre=-/sbin/modprobe overlay
ExecStart=/usr/local/bin/containerd

Type=notify
Delegate=yes
KillMode=process
Restart=always
RestartSec=5
# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNPROC=infinity
LimitCORE=infinity
LimitNOFILE=infinity
# Comment TasksMax if your systemd version does not supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
OOMScoreAdjust=-999

[Install]
WantedBy=multi-user.target
```

```shell
cp containerd.service /etc/systemd/system/containerd.service
```



### 10 安装 runc

https://github.com/opencontainers/runc/releases

最新版本

https://github.com/opencontainers/runc/releases/download/v1.1.9/runc.amd64

下载 runc.amd64 上传到服务器, 执行:

```shell
install -m 755 runc.amd64 /usr/local/sbin/runc
```



### 11 安装 CNI Plugins

https://github.com/containernetworking/plugins/releases

最新版本

https://github.com/containernetworking/plugins/releases/download/v1.3.0/cni-plugins-linux-amd64-v1.3.0.tgz

下载后上传到服务器

```shell
mkdir -p /opt/cni/bin
tar -xzvf cni-plugins-linux-amd64-v1.3.0.tgz -C /opt/cni/bin
```



### 12 安装 nerdctl

https://github.com/containerd/nerdctl

https://github.com/containerd/nerdctl/releases

最新版本:

https://github.com/containerd/nerdctl/releases/download/v1.5.0/nerdctl-1.5.0-linux-amd64.tar.gz

将下载的 nerdctl-1.5.0-linux-amd64.tar.gz 文件上传至服务器

解压:

```shell
tar -xzvf nerdctl-1.5.0-linux-amd64.tar.gz -C /usr/local/bin
```



### 13 配置 containerd

创建默认配置文件:

```shell
containerd config default > config.toml
```



使用 systemd 作为 cgroup  

修改配置文件中的 

```shell
SystemdCgroup = false
```

修改为 

```shell
SystemdCgroup = true
```



```shell
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' config.toml
```



配置 containerd 镜像加速

修改 /etc/containerd/config.toml

```shell
 [plugins."io.containerd.grpc.v1.cri".registry]
      config_path = ""
```

修改为

```shell
 [plugins."io.containerd.grpc.v1.cri".registry]
      config_path = "/etc/containerd/certs.d"
```

执行脚本完成自动修改配置文件

```shell
#!/bin/bash

sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' config.toml

# Step 1: Set initial state to "search"
state="search"

# Step 2: Read the file line by line and output each line
while IFS= read -r line; do
    # Step 3: If in "search" state, check for match and switch to "ready to modify" state
    if [ "$state" = "search" ]; then
        stripped_line=$(echo "$line" | tr -d '[:space:]')  # Remove spaces
        if [ "$stripped_line" = '[plugins."io.containerd.grpc.v1.cri".registry]' ]; then
            state="ready_to_modify"
        fi
    fi

    # Step 4: If in "ready to modify" state, update lines accordingly
    if [ "$state" = "ready_to_modify" ]; then
        stripped_line=$(echo "$line" | tr -d '[:space:]')  # Remove spaces
        
        if [ "$stripped_line" = 'config_path=""' ]; then
            modified_line="${line//\"\"/\"/etc/containerd/certs.d\"}"
            line="$modified_line"
            state="done"
        elif [ "${line:0:1}" = "[" ]; then
            state="done"
        fi
    fi

    echo "$line"
done < config.toml > config_tmp.toml

rm config.toml
mv config_tmp.toml config.toml

echo -e "\nScript completed."
```



修改配置完成后, 复制到指定位置:

```shell
mkdir /etc/containerd
cp  config.toml /etc/containerd/config.toml
```



```shell
mkdir /etc/containerd/certs.d
```

```shell
# docker hub镜像加速
mkdir -p /etc/containerd/certs.d/docker.io
cat > /etc/containerd/certs.d/docker.io/hosts.toml << EOF
server = "https://docker.io"
[host."https://dockerproxy.com"]
  capabilities = ["pull", "resolve"]

[host."https://docker.m.daocloud.io"]
  capabilities = ["pull", "resolve"]

[host."https://reg-mirror.qiniu.com"]
  capabilities = ["pull", "resolve"]

[host."https://registry.docker-cn.com"]
  capabilities = ["pull", "resolve"]

[host."http://hub-mirror.c.163.com"]
  capabilities = ["pull", "resolve"]

EOF

# registry.k8s.io镜像加速
mkdir -p /etc/containerd/certs.d/registry.k8s.io
tee /etc/containerd/certs.d/registry.k8s.io/hosts.toml << 'EOF'
server = "https://registry.k8s.io"

[host."https://k8s.m.daocloud.io"]
  capabilities = ["pull", "resolve", "push"]
EOF

# docker.elastic.co镜像加速
mkdir -p /etc/containerd/certs.d/docker.elastic.co
tee /etc/containerd/certs.d/docker.elastic.co/hosts.toml << 'EOF'
server = "https://docker.elastic.co"

[host."https://elastic.m.daocloud.io"]
  capabilities = ["pull", "resolve", "push"]
EOF

# gcr.io镜像加速
mkdir -p /etc/containerd/certs.d/gcr.io
tee /etc/containerd/certs.d/gcr.io/hosts.toml << 'EOF'
server = "https://gcr.io"

[host."https://gcr.m.daocloud.io"]
  capabilities = ["pull", "resolve", "push"]
EOF

# ghcr.io镜像加速
mkdir -p /etc/containerd/certs.d/ghcr.io
tee /etc/containerd/certs.d/ghcr.io/hosts.toml << 'EOF'
server = "https://ghcr.io"

[host."https://ghcr.m.daocloud.io"]
  capabilities = ["pull", "resolve", "push"]
EOF

# k8s.gcr.io镜像加速
mkdir -p /etc/containerd/certs.d/k8s.gcr.io
tee /etc/containerd/certs.d/k8s.gcr.io/hosts.toml << 'EOF'
server = "https://k8s.gcr.io"

[host."https://k8s-gcr.m.daocloud.io"]
  capabilities = ["pull", "resolve", "push"]
EOF

# mcr.m.daocloud.io镜像加速
mkdir -p /etc/containerd/certs.d/mcr.microsoft.com
tee /etc/containerd/certs.d/mcr.microsoft.com/hosts.toml << 'EOF'
server = "https://mcr.microsoft.com"

[host."https://mcr.m.daocloud.io"]
  capabilities = ["pull", "resolve", "push"]
EOF

# nvcr.io镜像加速
mkdir -p /etc/containerd/certs.d/nvcr.io
tee /etc/containerd/certs.d/nvcr.io/hosts.toml << 'EOF'
server = "https://nvcr.io"

[host."https://nvcr.m.daocloud.io"]
  capabilities = ["pull", "resolve", "push"]
EOF

# quay.io镜像加速
mkdir -p /etc/containerd/certs.d/quay.io
tee /etc/containerd/certs.d/quay.io/hosts.toml << 'EOF'
server = "https://quay.io"

[host."https://quay.m.daocloud.io"]
  capabilities = ["pull", "resolve", "push"]
EOF

# registry.jujucharms.com镜像加速
mkdir -p /etc/containerd/certs.d/registry.jujucharms.com
tee /etc/containerd/certs.d/registry.jujucharms.com/hosts.toml << 'EOF'
server = "https://registry.jujucharms.com"

[host."https://jujucharms.m.daocloud.io"]
  capabilities = ["pull", "resolve", "push"]
EOF

# rocks.canonical.com镜像加速
mkdir -p /etc/containerd/certs.d/rocks.canonical.com
tee /etc/containerd/certs.d/rocks.canonical.com/hosts.toml << 'EOF'
server = "https://rocks.canonical.com"

[host."https://rocks-canonical.m.daocloud.io"]
  capabilities = ["pull", "resolve", "push"]
EOF
```



启动 `containerd` 服务

```shell
systemctl daemon-reload
systemctl enable --now containerd
```



## 10 PKI

https://baike.baidu.com/item/%E5%85%AC%E9%92%A5%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/10881894

Kubernetes 需要 PKI 才能执行以下操作：

Kubelet 的客户端证书，用于 API 服务器身份验证

API 服务器端点的证书

集群管理员的客户端证书，用于 API 服务器身份认证

API 服务器的客户端证书，用于和 Kubelet 的会话

API 服务器的客户端证书，用于和 etcd 的会话

控制器管理器的客户端证书 /kubeconfig，用于和 API 服务器的会话

调度器的客户端证书 /kubeconfig ，用于和 API 服务器的会话

前端代理 的客户端及服务端证书

说明： 只有当你运行 kube-proxy 并要支持 扩展 API 服务器 时，才需要 front-proxy 证书



etcd 还实现了双向 TLS 来对客户端和对其他对等节点进行身份验证

k8s官方文档:

https://kubernetes.io/zh-cn/docs/setup/best-practices/certificates/#%E9%9B%86%E7%BE%A4%E6%98%AF%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E8%AF%81%E4%B9%A6%E7%9A%84



学习证书： https://www.cnblogs.com/technology178/p/14094375.html



ca根配置  

```shell
vi ca-config.json
```



```json
{
  "signing": {
    "default": {
      "expiry": "87600h"
    },
    "profiles": {
      "server": {
        "expiry": "87600h",
        "usages": [
          "signing",
          "key encipherment",
          "server auth"
        ]
      },
      "client": {
        "expiry": "87600h",
        "usages": [
          "signing",
          "key encipherment",
          "client auth"
        ]
      },
      "peer": {
        "expiry": "87600h",
        "usages": [
          "signing",
          "key encipherment",
          "server auth",
          "client auth"
        ]
      },
      "kubernetes": {
        "expiry": "87600h",
        "usages": [
          "signing",
          "key encipherment",
          "server auth",
          "client auth"
        ]
      },
      "etcd": {
        "expiry": "87600h",
        "usages": [
          "signing",
          "key encipherment",
          "server auth",
          "client auth"
        ]
      }
    }
  }
}
```





ca签名请求

CSR是Certificate Signing Request的英文缩写，即证书签名请求文件

```shell
vi k8s-ca-csr.json
```

```json
{
  "CN": "kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Beijing",
      "L": "Beijing",
      "O": "Kubernetes",
      "OU": "Kubernetes"
    }
  ],
  "ca": {
    "expiry": "87600h"
  }
}
```

CN(Common Name):

公用名（Common Name）必须填写，一般可以是网站域



O(Organization):Organization

（组织名）是必须填写的，如果申请的是OV、EV型证书，组织名称必须严格和企业在政府登记名称一致，一般需要和营业执照上的名称完全一致。不可以使用缩写或者商标。如果需要使用英文名称，需要有DUNS编码或者律师信证明。



OU(Organization Unit)

OU单位部门，这里一般没有太多限制，可以直接填写 IT DEPT 等皆可。



C(City)

City是指申请单位所在的城市。



ST(State/Province)

ST 是指申请单位所在的省份。



C(Country Name）

C是指国家名称，这里用的是两位大写的国家代码，中国是CN。



4 生成证书

生成ca证书和私钥

```shell
mkdir k8s-ca
```

```shell
cfssl gencert -initca k8s-ca-csr.json | cfssljson -bare ./k8s-ca/k8s-ca -
```

ca.csr 申请

ca.pem ca公钥

ca-key.pem  ca私钥,妥善保管



## 11 安装etcd

https://etcd.io/docs/v3.5/install/

https://github.com/etcd-io/etcd/releases/

https://github.com/etcd-io/etcd/releases/tag/v3.5.9

https://etcd.io/docs/next/op-guide/hardware/#small-cluster 安装参考  



当前最新版本为

https://github.com/etcd-io/etcd/releases/download/v3.5.9/etcd-v3.5.9-linux-amd64.tar.gz

下载最新版本的etcd安装包, 上传到服务器 etcd-v3.5.9-linux-amd64.tar.gz

解压:

```shell
tar -zxvf etcd-v3.5.9-linux-amd64.tar.gz --strip-components=1 -C /usr/local/bin etcd-v3.5.9-linux-amd64/etcd{,ctl}
```

也可以

```shell
tar -zxvf etcd-v3.5.9-linux-amd64.tar.gz
cp ./etcd-v3.5.9-linux-amd64/etcd /usr/local/bin
cp ./etcd-v3.5.9-linux-amd64/etcdctl /usr/local/bin
```

在所有etcd节点安装:

```shell
vi etcd_install.sh
```

```shell
#!/bin/bash

# 列出所有节点的 SSH 地址
etcd_nodes=(
  "k8s-master01"
  "k8s-master02"
  "k8s-master03"
)

# 要复制的文件路径
source_etcd="./etcd-v3.5.9-linux-amd64/etcd"
source_etcdctl="./etcd-v3.5.9-linux-amd64/etcdctl"
destination_dir="/usr/local/bin/"

tar -zxvf etcd-v3.5.9-linux-amd64.tar.gz

# 循环遍历所有节点，并复制文件
for node in "${etcd_nodes[@]}"; do
  echo "Copying files to $node..."
  scp "$source_etcd" "$source_etcdctl" "root@$node:$destination_dir"
  echo "Files copied to $node."
done

echo "Files copied to all nodes."
```

```shell
chmod +x etcd_install.sh
```

```shell
./etcd_install.sh
```





### 3 生成 etcd证书  CA 证书

```shell
vi etcd-ca-csr.json
```

```shell
{
  "CN": "etcd",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Beijing",
      "L": "Beijing",
      "O": "etcd",
      "OU": "etcd"
    }
  ],
  "ca": {
    "expiry": "87600h"
  }
}
```

生成etcd根ca证书

```shell
cfssl gencert -initca etcd-ca-csr.json | cfssljson -bare /etc/kubernetes/pki/etcd/ca -
```

这个命令会在 /etc/kubernetes/pki/etcd 目录下生成证书的名为 ca , 会生成三个文件, 分别是 .pem -key.pem .csr , 也就是 ca.pem ca.csr ca-key.pem 文件, 为了方便所有节点都分发证书, 可以在当前目录下生成取名为 ca 的证书

```shell
cfssl gencert -initca etcd-ca-csr.json | cfssljson -bare ./etcd-ca/etcd-ca -
```



4 生成 etcd 客户端证书

```shell
vi etcd-client-csr.json
```

```json
{
  "CN": "etcd-client",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "hosts": [
    "127.0.0.1",
    "k8s-master01",
    "k8s-master02",
    "k8s-master03",
    "192.168.0.151",
    "192.168.0.152",
    "192.168.0.153"
  ],
  "names": [
    {
      "C": "CN",
      "L": "beijing",
      "O": "etcd",
      "ST": "beijing",
      "OU": "System"
    }
  ]
}
```

注意：hosts用自己的主机名和ip
也可以在签发的时候再加上 -hostname=127.0.0.1,k8s-master1,k8s-master2,k8s-master3,
可以指定受信的主机列表

```json
"hosts": [
	"k8s-master1",
	"www.example.net"
],
```



生成证书:

```shell
cfssl gencert -ca=./etcd-ca/etcd-ca.pem -ca-key=./etcd-ca/etcd-ca-key.pem -config=./ca-config.json -profile=etcd etcd-client-csr.json | cfssljson -bare ./etcd-client/etcd-client
```



将证书分发到各个 etcd 节点

所有 etcd 节点执行:

```shell
mkdir -p /etc/kubernetes/pki
mkdir -p /etc/kubernetes/pki/etcd-ca
mkdir -p /etc/kubernetes/pki/etcd-client
```



在证书生成节点 k8s-master01 执行:

```shell
scp -r ./etcd-ca/* k8s-master01:/etc/kubernetes/pki/etcd-ca/
scp -r ./etcd-client/* k8s-master01:/etc/kubernetes/pki/etcd-client/

scp -r ./etcd-ca/* k8s-master02:/etc/kubernetes/pki/etcd-ca/
scp -r ./etcd-client/* k8s-master02:/etc/kubernetes/pki/etcd-client/

scp -r ./etcd-ca/* k8s-master03:/etc/kubernetes/pki/etcd-ca/
scp -r ./etcd-client/* k8s-master03:/etc/kubernetes/pki/etcd-client/
```



```shell
#!/bin/bash

# Define directory list and target nodes
directories=("etcd-ca" "etcd-client")
nodes=("k8s-master01" "k8s-master02" "k8s-master03")
target_directory="/etc/kubernetes/pki"

# Loop through each target node
for node in "${nodes[@]}"; do
    echo "Copying files to $node..."
    
    # Loop through each directory
    for directory in "${directories[@]}"; do
        echo "Copying from $directory..."
        
        # Extract the directory name from the path
        dirname=$(basename "$directory")
        
        # Copy files using scp to target directory with same name
        scp -r ./"$directory"/* "$node":"$target_directory"/"$dirname"/
        
        echo "Copy from $directory to $node completed."
    done
    
    echo "Copying to $node completed."
done

echo "Script completed."
```



### 4 etcd 高可用安装

etcd配置文件示例： https://etcd.io/docs/v3.5/op-guide/configuration/ 

https://etcd.io/docs/v3.5/op-guide/configuration/#configuration-file

https://github.com/etcd-io/etcd/blob/main/etcd.conf.yml.sample

```yaml
# This is the configuration file for the etcd server.

# Human-readable name for this member.
name: 'default'

# Path to the data directory.
data-dir:

# Path to the dedicated wal directory.
wal-dir:

# Number of committed transactions to trigger a snapshot to disk.
snapshot-count: 10000

# Time (in milliseconds) of a heartbeat interval.
heartbeat-interval: 100

# Time (in milliseconds) for an election to timeout.
election-timeout: 1000

# Raise alarms when backend size exceeds the given quota. 0 means use the
# default quota.
quota-backend-bytes: 0

# List of comma separated URLs to listen on for peer traffic.
listen-peer-urls: http://localhost:2380

# List of comma separated URLs to listen on for client traffic.
listen-client-urls: http://localhost:2379

# Maximum number of snapshot files to retain (0 is unlimited).
max-snapshots: 5

# Maximum number of wal files to retain (0 is unlimited).
max-wals: 5

# Comma-separated white list of origins for CORS (cross-origin resource sharing).
cors:

# List of this member's peer URLs to advertise to the rest of the cluster.
# The URLs needed to be a comma-separated list.
initial-advertise-peer-urls: http://localhost:2380

# List of this member's client URLs to advertise to the public.
# The URLs needed to be a comma-separated list.
advertise-client-urls: http://localhost:2379

# Discovery URL used to bootstrap the cluster.
discovery:

# Valid values include 'exit', 'proxy'
discovery-fallback: 'proxy'

# HTTP proxy to use for traffic to discovery service.
discovery-proxy:

# DNS domain used to bootstrap initial cluster.
discovery-srv:

# Comma separated string of initial cluster configuration for bootstrapping.
# Example: initial-cluster: "infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380"
initial-cluster:

# Initial cluster token for the etcd cluster during bootstrap.
initial-cluster-token: 'etcd-cluster'

# Initial cluster state ('new' or 'existing').
initial-cluster-state: 'new'

# Reject reconfiguration requests that would cause quorum loss.
strict-reconfig-check: false

# Enable runtime profiling data via HTTP server
enable-pprof: true

# Valid values include 'on', 'readonly', 'off'
proxy: 'off'

# Time (in milliseconds) an endpoint will be held in a failed state.
proxy-failure-wait: 5000

# Time (in milliseconds) of the endpoints refresh interval.
proxy-refresh-interval: 30000

# Time (in milliseconds) for a dial to timeout.
proxy-dial-timeout: 1000

# Time (in milliseconds) for a write to timeout.
proxy-write-timeout: 5000

# Time (in milliseconds) for a read to timeout.
proxy-read-timeout: 0

client-transport-security:
  # Path to the client server TLS cert file.
  cert-file:

  # Path to the client server TLS key file.
  key-file:

  # Enable client cert authentication.
  client-cert-auth: false

  # Path to the client server TLS trusted CA cert file.
  trusted-ca-file:

  # Client TLS using generated certificates
  auto-tls: false

peer-transport-security:
  # Path to the peer server TLS cert file.
  cert-file:

  # Path to the peer server TLS key file.
  key-file:

  # Enable peer client cert authentication.
  client-cert-auth: false

  # Path to the peer server TLS trusted CA cert file.
  trusted-ca-file:

  # Peer TLS using generated certificates.
  auto-tls: false

# The validity period of the self-signed certificate, the unit is year.
self-signed-cert-validity: 1

# Enable debug-level logging for etcd.
log-level: debug

logger: zap

# Specify 'stdout' or 'stderr' to skip journald logging even when running under systemd.
log-outputs: [stderr]

# Force to create a new one member cluster.
force-new-cluster: false

auto-compaction-mode: periodic
auto-compaction-retention: "1"

# Limit etcd to a specific set of tls cipher suites
cipher-suites: [
  TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,
  TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
]

# Limit etcd to specific TLS protocol versions 
tls-min-version: 'TLS1.2'
tls-max-version: 'TLS1.3'
```



三个etcd机器都创建 /etc/etcd 目录，准备存储etcd配置信息  

```shell
#三个master执行
mkdir -p /etc/etcd
```

```shell
vi /etc/etcd/etcd.yaml
```

```shell
name: 'etcd-master01' #每个机器可以写自己的域名,不能重复 'etcd-master01' 'etcd-master02' 'etcd-master03'
listen-peer-urls: 'https://192.168.0.151:2380' # 本机ip+2380端口，代表和集群通信
```

其中 etcd-node-01 的配置文件如下:

```yaml
name: 'etcd-node-01'
data-dir: /var/lib/etcd
wal-dir: /var/lib/etcd/wal
snapshot-count: 5000
heartbeat-interval: 100
election-timeout: 1000
quota-backend-bytes: 0
listen-peer-urls: 'https://192.168.0.151:2380'
listen-client-urls: 'https://192.168.0.151:2379,http://127.0.0.1:2379'
max-snapshots: 3
max-wals: 5
cors:
initial-advertise-peer-urls: 'https://192.168.0.151:2380'
advertise-client-urls: 'https://192.168.0.151:2379'
discovery:
discovery-fallback: 'proxy'
discovery-proxy:
discovery-srv:
initial-cluster: 'etcd-node-01=https://192.168.0.151:2380,etcd-node-02=https://192.168.0.152:2380,etcd-node-03=https://192.168.0.153:2380'
initial-cluster-token: 'etcd-k8s-cluster'
initial-cluster-state: 'new'
strict-reconfig-check: false
enable-v2: true
enable-pprof: true
proxy: 'off'
proxy-failure-wait: 5000
proxy-refresh-interval: 30000
proxy-dial-timeout: 1000
proxy-write-timeout: 5000
proxy-read-timeout: 0
client-transport-security:
  cert-file: '/etc/kubernetes/pki/etcd-client/etcd-client.pem'
  key-file: '/etc/kubernetes/pki/etcd-client/etcd-client-key.pem'
  client-cert-auth: true
  trusted-ca-file: '/etc/kubernetes/pki/etcd-ca/etcd-ca.pem'
  auto-tls: true
peer-transport-security:
  cert-file: '/etc/kubernetes/pki/etcd-client/etcd-client.pem'
  key-file: '/etc/kubernetes/pki/etcd-client/etcd-client-key.pem'
  peer-client-cert-auth: true
  trusted-ca-file: '/etc/kubernetes/pki/etcd-ca/etcd-ca.pem'
  auto-tls: true
debug: false
log-package-levels:
log-outputs: [default]
force-new-cluster: false
```

可以使用脚本为每一个节点生成配置文件:

```shell
vi etcd_config_gen.sh
```

```shell
#!/bin/bash

# Define etcd node list with names and IP addresses
etcd_nodes=(
    "etcd-node-01|192.168.0.151"
    "etcd-node-02|192.168.0.152"
    "etcd-node-03|192.168.0.153"
)

# Generate initial-cluster value
initial_cluster=""
for node in "${etcd_nodes[@]}"; do
    IFS='|' read -ra node_info <<< "$node"
    node_name="${node_info[0]}"
    node_ip="${node_info[1]}"
    
    if [ -n "$initial_cluster" ]; then
        initial_cluster+=","
    fi
    initial_cluster+="$node_name=https://$node_ip:2380"
done

# Loop through each etcd node
for node in "${etcd_nodes[@]}"; do
    IFS='|' read -ra node_info <<< "$node"
    node_name="${node_info[0]}"
    node_ip="${node_info[1]}"
    
    # Generate configuration content
    config_content="name: '$node_name'\n"
    config_content+="data-dir: /var/lib/etcd\n"
    config_content+="wal-dir: /var/lib/etcd/wal\n"
    config_content+="snapshot-count: 5000\n"
    config_content+="heartbeat-interval: 100\n"
    config_content+="election-timeout: 1000\n"
    config_content+="quota-backend-bytes: 0\n"
    config_content+="listen-peer-urls: 'https://$node_ip:2380'\n"
    config_content+="listen-client-urls: 'https://$node_ip:2379,http://127.0.0.1:2379'\n"
    config_content+="max-snapshots: 3\n"
    config_content+="max-wals: 5\n"
    config_content+="cors:\n"
    config_content+="initial-advertise-peer-urls: 'https://$node_ip:2380'\n"
    config_content+="advertise-client-urls: 'https://$node_ip:2379'\n"
    config_content+="discovery:\n"
    config_content+="discovery-fallback: 'proxy'\n"
    config_content+="discovery-proxy:\n"
    config_content+="discovery-srv:\n"
    config_content+="initial-cluster: '$initial_cluster'\n"
    config_content+="initial-cluster-token: 'etcd-k8s-cluster'\n"
    config_content+="initial-cluster-state: 'new'\n"
    config_content+="strict-reconfig-check: false\n"
    config_content+="enable-v2: true\n"
    config_content+="enable-pprof: true\n"
    config_content+="proxy: 'off'\n"
    config_content+="proxy-failure-wait: 5000\n"
    config_content+="proxy-refresh-interval: 30000\n"
    config_content+="proxy-dial-timeout: 1000\n"
    config_content+="proxy-write-timeout: 5000\n"
    config_content+="proxy-read-timeout: 0\n"
    config_content+="client-transport-security:\n"
    config_content+="  cert-file: '/etc/kubernetes/pki/etcd-client/etcd-client.pem'\n"
    config_content+="  key-file: '/etc/kubernetes/pki/etcd-client/etcd-client-key.pem'\n"
    config_content+="  client-cert-auth: true\n"
    config_content+="  trusted-ca-file: '/etc/kubernetes/pki/etcd-ca/etcd-ca.pem'\n"
    config_content+="  auto-tls: true\n"
    config_content+="peer-transport-security:\n"
    config_content+="  cert-file: '/etc/kubernetes/pki/etcd-client/etcd-client.pem'\n"
    config_content+="  key-file: '/etc/kubernetes/pki/etcd-client/etcd-client-key.pem'\n"
    config_content+="  peer-client-cert-auth: true\n"
    config_content+="  trusted-ca-file: '/etc/kubernetes/pki/etcd-ca/etcd-ca.pem'\n"
    config_content+="  auto-tls: true\n"
    config_content+="debug: false\n"
    config_content+="log-package-levels:\n"
    config_content+="log-outputs: [default]\n"
    config_content+="force-new-cluster: false\n"
    
    # Create configuration file
    config_file="$node_name-config.yaml"
    echo -e "$config_content" > "$config_file"
    
    echo "Configuration file '$config_file' created for $node_name"
done

echo "Script completed."
```

生成配置文件如下:

```shell
root@k8s-master01:~/k8s_install/etcd/config# ls
config_gen.sh  etcd-node-01-config.yaml  etcd-node-02-config.yaml  etcd-node-03-config.yaml
```

将配置文件发送到各个节点:

```shell
vi etcd_config_copy.sh
```

```shell
#!/bin/bash

# Define node-to-etcd mapping
node_to_etcd=(
    "k8s-master01|etcd-node-01"
    "k8s-master02|etcd-node-02"
    "k8s-master03|etcd-node-03"
)

# Loop through each node
for mapping in "${node_to_etcd[@]}"; do
    IFS='|' read -ra map_info <<< "$mapping"
    node_name="${map_info[0]}"
    etcd_node="${map_info[1]}"
    
    # Check if remote directory exists and create if not
    ssh "$node_name" "mkdir -p /etc/etcd"
    
    # Copy configuration file to remote directory
    scp "$etcd_node-config.yaml" "$node_name:/etc/etcd/etcd.yaml"
    
    echo "Configuration file copied to $node_name for $etcd_node"
done

echo "Script completed."
```



etcd高可用安装示例： https://etcd.io/docs/v3.5/op-guide/clustering/

为了保证启动配置一致性，我们编写etcd配置文件，并将etcd做成service启动

三台机器的etcd做成service，开机启动  

```shell
vi etcd.service
```

```toml
[Unit]
Description=Etcd Service
Documentation=https://etcd.io/docs/v3.5/op-guide/clustering/
After=network.target
[Service]
Type=notify
ExecStart=/usr/local/bin/etcd --config-file=/etc/etcd/etcd.yaml
Restart=on-failure
RestartSec=10
LimitNOFILE=65536
[Install]
WantedBy=multi-user.target
Alias=etcd3.service
```

使用脚本将service文件发送到各个节点的指定目录 /usr/lib/systemd/system/etcd.service

```shell
#!/bin/bash

# Define node-to-etcd mapping
etcd_nodes=(
    "k8s-master01"
    "k8s-master02"
    "k8s-master03"
)

# Loop through each node
for node in "${etcd_nodes[@]}"; do
    # Copy service file to remote directory
    scp "etcd.service" "$node:/usr/lib/systemd/system/etcd.service"
    
    echo "service file copied to $node"
done

echo "Script completed."
```



加载&开机启动

所有 etcd 节点执行:

```shell
systemctl daemon-reload
systemctl enable --now etcd
```

```shell
# 启动有问题,使用 journalctl -u 服务名排查
journalctl -xef -u etcd
```



测试etcd访问  

查看etcd集群状态

在所有 etcd 节点上执行:

```shell
etcdctl --endpoints="192.168.0.151:2379,192.168.0.152:2379,192.168.0.153:2379" --cacert=/etc/kubernetes/pki/etcd-ca/etcd-ca.pem --cert=/etc/kubernetes/pki/etcd-client/etcd-client.pem --key=/etc/kubernetes/pki/etcd-client/etcd-client-key.pem endpoint status --write-out=table
```

可以看到etcd状态输出:

```
root@k8s-master01:~# etcdctl --endpoints="192.168.0.151:2379,192.168.0.152:2379,192.168.0.153:2379" --cacert=/etc/kubernetes/pki/etcd-ca/etcd-ca.pem --cert=/etc/kubernetes/pki/etcd-client/etcd-client.pem --key=/etc/kubernetes/pki/etcd-client/etcd-client-key.pem endpoint status --write-out=table
+--------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|      ENDPOINT      |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |
+--------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
| 192.168.0.151:2379 |  421300155c3e1ef |   3.5.9 |   20 kB |     false |      false |         2 |          8 |                  8 |        |
| 192.168.0.152:2379 | 42befda33887f3c8 |   3.5.9 |   25 kB |     false |      false |         2 |          8 |                  8 |        |
| 192.168.0.153:2379 | b73a69b41458e902 |   3.5.9 |   20 kB |      true |      false |         2 |          8 |                  8 |        |
+--------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
```

![image-20230822232646716](assets/008_k8s_二进制高可用集群/image-20230822232646716.png)

导出环境变量，方便测试，参照 https://github.com/etcd-io/etcd/tree/main/etcdctl



```shell
export ETCDCTL_API=3
HOST_1=192.168.0.151
HOST_2=192.168.0.152
HOST_3=192.168.0.153
ENDPOINTS=$HOST_1:2379,$HOST_2:2379,$HOST_3:2379

export ETCDCTL_DIAL_TIMEOUT=3s
export ETCDCTL_CACERT=/etc/kubernetes/pki/etcd-ca/etcd-ca.pem
export ETCDCTL_CERT=/etc/kubernetes/pki/etcd-client/etcd-client.pem
export ETCDCTL_KEY=/etc/kubernetes/pki/etcd-client/etcd-client-key.pem
export ETCDCTL_ENDPOINTS=$HOST_1:2379,$HOST_2:2379,$HOST_3:2379
```

自动用环境变量定义的证书位置

```shell
etcdctl member list --write-out=table
```

```shell
root@k8s-master01:~# etcdctl member list --write-out=table
+------------------+---------+--------------+----------------------------+----------------------------+------------+
|        ID        | STATUS  |     NAME     |         PEER ADDRS         |        CLIENT ADDRS        | IS LEARNER |
+------------------+---------+--------------+----------------------------+----------------------------+------------+
|  421300155c3e1ef | started | etcd-node-01 | https://192.168.0.151:2380 | https://192.168.0.151:2379 |      false |
| 42befda33887f3c8 | started | etcd-node-02 | https://192.168.0.152:2380 | https://192.168.0.152:2379 |      false |
| b73a69b41458e902 | started | etcd-node-03 | https://192.168.0.153:2380 | https://192.168.0.153:2379 |      false |
+------------------+---------+--------------+----------------------------+----------------------------+------------+
```

如果没有使用环境变量就需要如下方式调用, 注意如果设置了环境变量, 下面的命令会报错, 原因是 etcdctl 命令参数和环境变量中都读出参数的值导致冲突

```shell
etcdctl --endpoints=$ENDPOINTS --cacert=/etc/kubernetes/pki/etcd/ca.pem --cert=/etc/kubernetes/pki/etcd/etcd.pem --key=/etc/kubernetes/pki/etcd/etcdkey.pem member list --write-out=table
```

更多etcdctl命令，https://etcd.io/docs/v3.5/demo/#access-etcd



## 12 NGINX高可用部署



### 1 进行编译

在 load-balancer 节点执行

```shell
apt update && apt upgrade -y 

# 安装编译环境
apt install -y gcc make

# 创建目录
mkdir -p ~/setup/nginx
cd ~/setup/nginx

# 下载解压nginx二进制文件
# wget http://nginx.org/download/nginx-1.25.2.tar.gz
# 或者在其他服务器上下载后上传到当前目录
tar -zxvf nginx-1.25.2.tar.gz
cd nginx-1.25.2

# 进行编译
./configure --with-stream --without-http --without-http_uwsgi_module --without-http_scgi_module --without-http_fastcgi_module make && make install

# 拷贝编译好的nginx
nginx_nodes=(
    "k8s-loadbalancer"
)
for NODE in $nginx_nodes; do 
	scp -r /usr/local/nginx/ $NODE:/usr/local/nginx/; 
done

# 这是一系列命令行指令，用于编译和安装软件。
# 
# 1. `./configure` 是用于配置软件的命令。在这个例子中，配置的软件是一个Web服务器，指定了一些选项来启用流模块，并禁用了HTTP、uwsgi、scgi和fastcgi模块。
# 2. `--with-stream` 指定启用流模块。流模块通常用于代理TCP和UDP流量。
# 3. `--without-http` 指定禁用HTTP模块。这意味着编译的软件将没有HTTP服务器功能。
# 4. `--without-http_uwsgi_module` 指定禁用uwsgi模块。uwsgi是一种Web服务器和应用服务器之间的通信协议。
# 5. `--without-http_scgi_module` 指定禁用scgi模块。scgi是一种用于将Web服务器请求传递到应用服务器的协议。
# 6. `--without-http_fastcgi_module` 指定禁用fastcgi模块。fastcgi是一种用于在Web服务器和应用服务器之间交换数据的协议。
# 7. `make` 是用于编译软件的命令。该命令将根据之前的配置生成可执行文件。
# 8. `make install` 用于安装软件。该命令将生成的可执行文件和其他必要文件复制到系统的适当位置，以便可以使用该软件。
# 
# 总之，这个命令序列用于编译一个配置了特定选项的Web服务器，并将其安装到系统中。
```



也可以在k8s-master01上执行

```shell
mkdir load_balancer
cd load_balancer
```



下载 nginx 安装包, 或者由其他节点上传至当前路径

```shell
wget http://nginx.org/download/nginx-1.25.2.tar.gz
```

```shell
tar -zxvf nginx-1.25.2.tar.gz
cd nginx-1.25.2
```

安装 gcc 和 make

```
apt install -y gcc make
```

```shell
./configure --with-stream --without-http --without-http_uwsgi_module --without-http_scgi_module --without-http_fastcgi_module
```

```shell
make && make install
```

```shell
cd ..
```

将 nginx 分发到 nginx 节点

```shell
vi nginx_copy.sh
```

```shell
nginx_nodes=(
    "k8s-loadbalancer"
)

for NODE in "${nginx_nodes[@]}"; do 
	scp -r /usr/local/nginx/ $NODE:/usr/local/nginx/; 
done
```

```shell
chmod +x nginx_copy.sh
```

```shell
./nginx_copy.sh
```



```sshell
ssh root@k8s-loadbalancer "apt update && apt upgrade -y"
```





### 2 写入启动配置

在所有 nginx 主机上执行

```shell
# 写入nginx配置文件
cat > /usr/local/nginx/conf/kube-nginx.conf <<EOF
worker_processes 1;
events {
    worker_connections  1024;
}
stream {
    upstream backend {
        least_conn;
        hash $remote_addr consistent;
        server 192.168.0.151:6443        max_fails=3 fail_timeout=30s;
        server 192.168.0.152:6443        max_fails=3 fail_timeout=30s;
        server 192.168.0.153:6443        max_fails=3 fail_timeout=30s;
    }
    server {
        listen 6443;
        proxy_connect_timeout 1s;
        proxy_pass backend;
    }
}
EOF
# 这段配置是一个nginx的stream模块的配置，用于代理TCP和UDP流量。
# 
# 首先，`worker_processes 1;`表示启动一个worker进程用于处理流量。
# 接下来，`events { worker_connections 1024; }`表示每个worker进程可以同时处理最多1024个连接。
# 在stream块里面，定义了一个名为`backend`的upstream，用于负载均衡和故障转移。
# `least_conn`表示使用最少连接算法进行负载均衡。
# `hash $remote_addr consistent`表示用客户端的IP地址进行哈希分配请求，保持相同IP的请求始终访问同一台服务器。
# `server`指令用于定义后端的服务器，每个服务器都有一个IP地址和端口号，以及一些可选的参数。
# `max_fails=3`表示当一个服务器连续失败3次时将其标记为不可用。
# `fail_timeout=30s`表示如果一个服务器被标记为不可用，nginx将在30秒后重新尝试。
# 在server块内部，定义了一个监听地址为127.0.0.1:8443的服务器。
# `proxy_connect_timeout 1s`表示与后端服务器建立连接的超时时间为1秒。
# `proxy_pass backend`表示将流量代理到名为backend的上游服务器组。
# 
# 总结起来，这段配置将流量代理到一个包含3个后端服务器的上游服务器组中，使用最少连接算法进行负载均衡，并根据客户端的IP地址进行哈希分配请求。如果一个服务器连续失败3次，则将其标记为不可用，并在30秒后重新尝试。
```

```
worker_processes 1;
events {
    worker_connections  1024;
}
stream {
    upstream backend {
        least_conn;
        hash  consistent;

        server 192.168.0.151:6443 max_fails=3 fail_timeout=30s;
        server 192.168.0.152:6443 max_fails=3 fail_timeout=30s;
        server 192.168.0.153:6443 max_fails=3 fail_timeout=30s;
    }
    server {
        listen 6443;
        proxy_connect_timeout 1s;
        proxy_pass backend;
    }
}
```

可以使用脚本生成上面的配置文件

```shell
vi nginx-conf-gen.sh
```

```shell
#!/bin/bash

# Define the list of nginx_nodes
k8s_master_nodes=(
    "192.168.0.151"
    "192.168.0.152"
    "192.168.0.153"
)

# Create nginx configuration
config_file="kube-nginx.conf"

echo "worker_processes 1;
events {
    worker_connections  1024;
}
stream {
    upstream backend {
        least_conn;
        hash $remote_addr consistent;
        " > "$config_file"

for node in "${k8s_master_nodes[@]}"; do
    echo "        server $node:6443 max_fails=3 fail_timeout=30s;" >> "$config_file"
done

echo "    }
    server {
        listen 127.0.0.1:8443;
        proxy_connect_timeout 1s;
        proxy_pass backend;
    }
}" >> "$config_file"

echo "Nginx configuration file '$config_file' has been generated."
```

```shell
chmod +x nginx-conf-gen.sh
```

```shell
./nginx-conf-gen.sh
```



将生成的配置文件发送至 nginx 节点

```shell
vi nginx-conf-copy.sh
```

```shell
#!/bin/bash

nodes=(
    "k8s-loadbalancer"
)

target_directory="/usr/local/nginx/conf/"

files=(
    "kube-nginx.conf"
)

for node in "${nodes[@]}"; do
    for file in $files; do
    	scp $file $node:$target_directory; 
    done
    
    echo "Copying to $node completed."
done

echo "Script completed."
```

```shell
chmod +x nginx-conf-copy.sh
```

```
./nginx-conf-copy.sh
```



写入启动配置文件

在 load-balancer 节点执行:

```shell
cat > /etc/systemd/system/kube-nginx.service <<EOF
[Unit]
Description=kube-apiserver nginx proxy
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
Type=forking
ExecStartPre=/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/kube-nginx.conf -p /usr/local/nginx -t
ExecStart=/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/kube-nginx.conf -p /usr/local/nginx
ExecReload=/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/kube-nginx.conf -p /usr/local/nginx -s reload
PrivateTmp=true
Restart=always
RestartSec=5
StartLimitInterval=0
LimitNOFILE=65536
 
[Install]
WantedBy=multi-user.target
EOF
```

或者在本机生成配置文件后, 分发给 load-balancer 节点

```shell
cat > kube-nginx.service <<EOF
[Unit]
Description=kube-apiserver nginx proxy
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
Type=forking
ExecStartPre=/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/kube-nginx.conf -p /usr/local/nginx -t
ExecStart=/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/kube-nginx.conf -p /usr/local/nginx
ExecReload=/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/kube-nginx.conf -p /usr/local/nginx -s reload
PrivateTmp=true
Restart=always
RestartSec=5
StartLimitInterval=0
LimitNOFILE=65536
 
[Install]
WantedBy=multi-user.target
EOF
```

将生成的 service 配置文件发送至 load-balancer 节点

```shell
vi nginx-service-copy.sh
```

```shell
#!/bin/bash

nodes=(
    "k8s-loadbalancer"
)

target_directory="/etc/systemd/system/"

files=(
    "kube-nginx.service"
)

for node in "${nodes[@]}"; do
    for file in $files; do
    	scp $file $node:$target_directory; 
    done
    
    echo "Copying to $node completed."
done

echo "Script completed."
```

```shell
chmod +x nginx-service-copy.sh
```

```
./nginx-service-copy.sh
```

这是一个用于kube-apiserver的NGINX代理的systemd单位文件。

[Unit]部分包含了单位的描述和依赖关系。它指定了在network.target和network-online.target之后启动，并且需要network-online.target。

[Service]部分定义了如何运行该服务。Type指定了服务进程的类型（forking表示主进程会派生一个子进程）。ExecStartPre指定了在服务启动之前需要运行的命令，用于检查NGINX配置文件的语法是否正确。ExecStart指定了启动服务所需的命令。ExecReload指定了在重新加载配置文件时运行的命令。PrivateTmp设置为true表示将为服务创建一个私有的临时文件系统。Restart和RestartSec用于设置服务的自动重启机制。StartLimitInterval设置为0表示无需等待，可以立即重启服务。LimitNOFILE指定了服务的文件描述符的限制。

[Install]部分指定了在哪些target下该单位应该被启用。

综上所述，此单位文件用于启动和管理 kube-apiserver 的 NGINX 代理服务。它通过 NGINX 来反向代理和负载均衡 kube-apiserver 的请求。该服务会在系统启动时自动启动，并具有自动重启的机制。



启动nginx服务并设置开机自启

在 load-balancer 节点执行:

```shell
systemctl daemon-reload
systemctl enable --now kube-nginx.service
```

或者在本机执行:

```shell
vi nginx-service-start.sh
```

```
#!/bin/bash

nodes=(
    "k8s-loadbalancer"
)

for node in $nodes; do
	ssh root@$node "systemctl daemon-reload && systemctl enable --now kube-nginx.service"
done

echo "Script completed."
```

```shell
chmod +x nginx-service-start.sh
```

```shell
./nginx-service-start.sh
```



```
# 用于重新加载systemd管理的单位文件。当你新增或修改了某个单位文件（如.service文件、.socket文件等），需要运行该命令来刷新systemd对该文件的配置。
systemctl enable --now kube-nginx.service
# 启用并立即启动kube-nginx.service单元。kube-nginx.service是kube-nginx守护进程的systemd服务单元。
systemctl restart kube-nginx.service
# 重启kube-nginx.service单元，即重新启动kube-nginx守护进程。
systemctl status kube-nginx.service
# kube-nginx.service单元的当前状态，包括运行状态、是否启用等信息。
```





## 12 k8s组件与证书  

### 1 下载 kubernetes

找到changelog对应版本  

https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.28.md

下载 kubernetes Server

https://dl.k8s.io/v1.28.0/kubernetes-server-linux-amd64.tar.gz



### 2 节点准备  

把kubernetes把复制给master所有节点

```shell
for i in k8s-master1 k8s-master2 k8s-master3 k8s-node1 k8s-node2 k8s-node3;do
scp kubernetes-server-* root@$i:/root/;done
```

master需要全部组件, 所有master节点解压kubelet \ kubectl \ kube--apiserver \ kube-controller-manager \ kube-scheduler \ kube-proxy 到 /usr/local/bin

```shell
tar -xvf kubernetes-server-linux-amd64.tar.gz --strip-components=3 -C ./bin kubernetes/server/bin/kube{let,ctl,-apiserver,-controller-manager,-scheduler,-proxy}
```

worker 节点只需要 /usr/local/bin kubelet、kube-proxy

```shell
tar -xvf kubernetes-server-linux-amd64.tar.gz --strip-components=3 -C
/usr/local/bin kubernetes/server/bin/kube{let,-proxy}
```



```shell
tar -xvf kubernetes-server-linux-amd64.tar.gz --strip-components=3 -C ./bin kubernetes/server/bin/kube{let,ctl,-apiserver,-controller-manager,-scheduler,-proxy}
```

将可执行文件分发到master节点和worker节点



```shell
#!/bin/bash

# Define k8s-master node list
k8s_master_nodes=(
    "k8s-master01"
    "k8s-master02"
    "k8s-master03"
)

k8s_master_bins=(
     "kubelet"
     "kubectl"
     "kube-apiserver"
     "kube-controller-manager"
     "kube-scheduler"
     "kube-proxy"
)

# Define k8s-worker node list
k8s_worker_nodes=(
    "k8s-worker01"
    "k8s-worker02"
    "k8s-worker03"
)

k8s_worker_bins=(
     "kubelet"
     "kube-proxy"
)

# Copy files to k8s-master nodes
for node in "${k8s_master_nodes[@]}"; do
    for bin_file in "${k8s_master_bins[@]}"; do
	    scp ./bin/$bin_file "$node:/usr/local/bin/"
    done
    
    echo "Files copied to $node (k8s-master)"
done

# Copy files to k8s-worker nodes
for node in "${k8s_worker_nodes[@]}"; do
    for bin_file in "${k8s_worker_bins[@]}"; do
	    scp ./bin/$bin_file "$node:/usr/local/bin/"
    done
    
    echo "Files copied to $node (k8s-worker)"
done

echo "Script completed."
```



### 3 生成k8s 的 CA机构

```shell
vi k8s-ca-csr.json
```

```json
{
  "CN": "kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Beijing",
      "L": "Beijing",
      "O": "Kubernetes",
      "OU": "Kubernetes"
    }
  ],
  "ca": {
    "expiry": "87600h"
  }
}
```

```shell
cfssl gencert -initca k8s-ca-csr.json | cfssljson -bare k8s-ca/k8s-ca -
```

将ca证书发给



### 3 apiserver 证书生成  

根据集群网络规划

10.96.0. 为service网段。可以自定义 如： 66.66.0.1

192.168.0.250： 是负载均衡器地址（负载均衡可以自己搭建，也可以购买云厂商lb。）

#### 1 创建证书申请书

```shell
vi k8s-apiserver-csr.json
```

```json
{
  "CN": "kube-apiserver",
  "hosts": [
    "10.96.0.1",
    "127.0.0.1",
    "192.168.0.250",
    "192.168.0.151",
    "192.168.0.152",
    "192.168.0.153",
    "192.168.0.161",
    "192.168.0.162",
    "192.168.0.163",
    "kubernetes",
    "kubernetes.default",
    "kubernetes.default.svc",
    "kubernetes.default.svc.cluster",
    "kubernetes.default.svc.cluster.local"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "BeiJing",
      "ST": "BeiJing",
      "O": "Kubernetes",
      "OU": "Kubernetes"
    }
  ]
}
```

#### 2 生成apiserver证书

192.168.0. 是k8s service的网段，如果说需要更改k8s service网段，那就需要更改
192.168.0.1，

如果不是高可用集群，10.103.236.236为Master01的IP

创建存储证书的目录

```shell
mkdir k8s-apiserver
```

生成证书

```shell
cfssl gencert -ca=k8s-ca/k8s-ca.pem -ca-key=k8s-ca/k8s-ca-key.pem -config=ca-config.json -profile=kubernetes k8s-apiserver-csr.json | cfssljson -bare k8s-apiserver/k8s-apiserver
```



### 4 front-proxy证书生成

配置[聚合层](https://kubernetes.io/zh-cn/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/)可以允许 Kubernetes apiserver 使用其它 API 扩展，这些 API 不是核心 Kubernetes API 的一部分。

https://kubernetes.io/zh-cn/docs/tasks/extend-kubernetes/configure-aggregation-layer/

他是apiserver聚合层，后来支持CRD(自定义的资源文件)的

```shell
apiVersion: xxx
kind: HelloDaChang --- CRD --- front-proxy
```

注意：front-proxy不建议用新的CA机构签发证书，可能导致通过他代理的组件如metrics-server权限不可用。  

如果用新的，api-server配置添加 --requestheader-allowed-names=front-proxy-client  

#### 1 创建 front-proxy 根 ca  

```shell
vi k8s-front-proxy-ca-csr.json
```

```json
{
  "CN": "kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  }
}
```



创建保存 front-proxy 的CA证书的目录

```shell
mkdir k8s-front-proxy-ca
```

生成 front-proxy 根ca

```shell
cfssl gencert -initca k8s-front-proxy-ca-csr.json | cfssljson -bare k8s-front-proxy-ca/k8s-front-proxy-ca
```



#### 2 front-proxy-client 证书  

准备申请client客户端证书

```shell
vi k8s-front-proxy-client-csr.json
```

```shell
{
  "CN": "front-proxy-client",
  "key": {
    "algo": "rsa",
    "size": 2048
  }
}
```



创建保存客户端证书的目录

```shell
mkdir k8s-front-proxy-client
```

生成 front-proxy-client 证书

```shell
cfssl gencert -ca=k8s-front-proxy-ca/k8s-front-proxy-ca.pem -ca-key=k8s-front-proxy-ca/k8s-front-proxy-ca-key.pem -config=ca-config.json -profile=kubernetes k8s-front-proxy-client-csr.json | cfssljson -bare k8s-front-proxy-client/k8s-front-proxy-client
```

忽略警告，不是给网站生成的不需要host字段



### 5 controller-manage证书生成与配置  

1 创建证书申请

```shell
vi k8s-controller-manager-csr.json
```

```json
{
  "CN": "system:kube-controller-manager",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Beijing",
      "L": "Beijing",
      "O": "system:kube-controller-manager",
      "OU": "Kubernetes"
    }
  ]
}
```



2 生成证书  

创建保存证书的目录

```shell
mkdir k8s-controller-manager
```

```shell
cfssl gencert -ca=k8s-ca/k8s-ca.pem -ca-key=k8s-ca/k8s-ca-key.pem -config=ca-config.json -profile=kubernetes k8s-controller-manager-csr.json | cfssljson -bare k8s-controller-manager/k8s-controller-manager
```

忽略警告, 不需要host字段



3 生成配置  

`kubectl config`   会在 --kubeconfig 指定的位置生成或者更新配置文件

注意，如果不是高可用集群，192.168.0.250:6443改为master01的地址，6443为apiserver的默认端口 

set-cluster：设置一个集群项

```shell
kubectl config set-cluster kubernetes \
--certificate-authority=/etc/kubernetes/pki/k8s-ca/k8s-ca.pem \
--embed-certs=true \
--server=https://192.168.0.250:6443 \
--kubeconfig=/etc/kubernetes/controller-manager.conf
```

或者在本地生成

```shell
kubectl config set-cluster kubernetes --certificate-authority=k8s-ca/k8s-ca.pem --embed-certs=true --server=https://192.168.0.250:6443 --kubeconfig=controller-manager.conf
```



设置一个环境项，一个上下文

```shell
kubectl config set-context system:kube-controller-manager@kubernetes \
--cluster=kubernetes \
--user=system:kube-controller-manager \
--kubeconfig=/etc/kubernetes/controller-manager.conf
```

或者本地生成

```shell
kubectl config set-context system:kube-controller-manager@kubernetes --cluster=kubernetes --user=system:kube-controller-manager --kubeconfig=controller-manager.conf
```

set-credentials 设置一个用户项

```shell
kubectl config set-credentials system:kube-controller-manager \
--client-certificate=/etc/kubernetes/pki/controller-manager.pem \
--client-key=/etc/kubernetes/pki/controller-manager-key.pem \
--embed-certs=true \
--kubeconfig=/etc/kubernetes/controller-manager.conf
```

或者本地生成

```shell
kubectl config set-credentials system:kube-controller-manager --client-certificate=k8s-controller-manager/k8s-controller-manager.pem --client-key=k8s-controller-manager/k8s-controller-manager-key.pem --embed-certs=true --kubeconfig=controller-manager.conf
```



使用某个环境当做默认环境  

```shell
kubectl config use-context system:kube-controller-manager@kubernetes \
--kubeconfig=/etc/kubernetes/controller-manager.conf
```

或者本地生成

```shell
kubectl config use-context system:kube-controller-manager@kubernetes --kubeconfig=controller-manager.conf
```

后来也用来自动批复kubelet证书  



### 6 scheduler证书生成与配置  

1 创建证书申请

```shell
vi k8s-scheduler-csr.json
```

```json
{
  "CN": "system:kube-scheduler",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Beijing",
      "L": "Beijing",
      "O": "system:kube-scheduler",
      "OU": "Kubernetes"
    }
  ]
}
```



2 签发证书  

```shell
mkdir k8s-scheduler
```

```shell
cfssl gencert -ca=k8s-ca/k8s-ca.pem -ca-key=k8s-ca/k8s-ca-key.pem -config=ca-config.json -profile=kubernetes k8s-scheduler-csr.json | cfssljson -bare k8s-scheduler/k8s-scheduler
```



3 生成配置

注意，如果不是高可用集群，192.168.0.250:6443 改为 master01 的地址，6443 是 api-server 默认端口

```shell
kubectl config set-cluster kubernetes \
--certificate-authority=/etc/kubernetes/pki/ca.pem \
--embed-certs=true \
--server=https://192.168.0.250:6443 \
--kubeconfig=/etc/kubernetes/scheduler.conf

kubectl config set-credentials system:kube-scheduler \
--client-certificate=/etc/kubernetes/pki/scheduler.pem \
--client-key=/etc/kubernetes/pki/scheduler-key.pem \
--embed-certs=true \
--kubeconfig=/etc/kubernetes/scheduler.conf

kubectl config set-context system:kube-scheduler@kubernetes \
--cluster=kubernetes \
--user=system:kube-scheduler \
--kubeconfig=/etc/kubernetes/scheduler.conf

kubectl config use-context system:kube-scheduler@kubernetes \
--kubeconfig=/etc/kubernetes/scheduler.conf
```

或者在本地生成

```shell
kubectl config set-cluster kubernetes --certificate-authority=k8s-ca/k8s-ca.pem --embed-certs=true --server=https://192.168.0.250:6443 --kubeconfig=scheduler.conf

kubectl config set-credentials system:kube-scheduler --client-certificate=k8s-scheduler/k8s-scheduler.pem --client-key=k8s-scheduler/k8s-scheduler-key.pem --embed-certs=true --kubeconfig=scheduler.conf

kubectl config set-context system:kube-scheduler@kubernetes --cluster=kubernetes --user=system:kube-scheduler --kubeconfig=scheduler.conf

kubectl config use-context system:kube-scheduler@kubernetes --kubeconfig=scheduler.conf
```

k8s集群安全操作相关  



### 7 admin证书生成与配置  

1 证书申请

```shell
vi k8s-admin-csr.json
```

```json
{
  "CN": "admin",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Beijing",
      "L": "Beijing",
      "O": "system:masters",
      "OU": "Kubernetes"
    }
  ]
}
```



2 生成证书  

```shell
mkdir k8s-admin
```

```shell
cfssl gencert -ca=k8s-ca/k8s-ca.pem -ca-key=k8s-ca/k8s-ca-key.pem -config=ca-config.json -profile=kubernetes k8s-admin-csr.json | cfssljson -bare k8s-admin/k8s-admin
```



3 生成配置

注意，如果不是高可用集群，192.168.0.250:6443 改为master01的地址，6443为apiserver的默认端口  

```shell
kubectl config set-cluster kubernetes \
--certificate-authority=/etc/kubernetes/pki/ca.pem \
--embed-certs=true \
--server=https://192.168.0.250:6443 \
--kubeconfig=/etc/kubernetes/admin.conf

kubectl config set-credentials kubernetes-admin \
--client-certificate=/etc/kubernetes/pki/admin.pem \
--client-key=/etc/kubernetes/pki/admin-key.pem \
--embed-certs=true \
--kubeconfig=/etc/kubernetes/admin.conf

kubectl config set-context kubernetes-admin@kubernetes \
--cluster=kubernetes \
--user=kubernetes-admin \
--kubeconfig=/etc/kubernetes/admin.conf

kubectl config use-context kubernetes-admin@kubernetes \
--kubeconfig=/etc/kubernetes/admin.conf
```

或者在本地生成

```shell
kubectl config set-cluster kubernetes --certificate-authority=k8s-ca/k8s-ca.pem --embed-certs=true --server=https://192.168.0.250:6443 --kubeconfig=admin.conf

kubectl config set-credentials kubernetes-admin --client-certificate=k8s-admin/k8s-admin.pem --client-key=k8s-admin/k8s-admin-key.pem --embed-certs=true --kubeconfig=admin.conf

kubectl config set-context kubernetes-admin@kubernetes --cluster=kubernetes --user=kubernetes-admin --kubeconfig=admin.conf

kubectl config use-context kubernetes-admin@kubernetes --kubeconfig=admin.conf
```



kubelet将使用 bootstrap 引导机制，自动颁发证书，所以不用配置了



### 8 ServiceAccount Key生成  

k8s底层，每创建一个ServiceAccount，都会分配一个Secret，而Secret里面有秘钥，秘钥就是由我们接下来的sa生成的。所以我们提前创建出sa信息  

```shell
openssl genrsa -out /etc/kubernetes/pki/sa.key 2048

openssl rsa -in /etc/kubernetes/pki/sa.key -pubout -out /etc/kubernetes/pki/sa.pub
```

或者在本地生成

```shell
mkdir k8s-service-account
openssl genrsa -out k8s-service-account/k8s-service-account.key 2048
openssl rsa -in k8s-service-account/k8s-service-account.key -pubout -out k8s-service-account/k8s-service-account.pub
```



### 9 发送证书到其他节点  

在master1上执行

```shell
for NODE in k8s-master2 k8s-master3
do
    for FILE in admin.conf controller-manager.conf scheduler.conf
    do
	    scp /etc/kubernetes/${FILE} $NODE:/etc/kubernetes/${FILE}
    done
done
```

将当前目录的所有证书和配置文件发送到各个 master 节点

```shell
vi k8s-master-certs-copy.sh
```

```shell
#!/bin/bash

# Define k8s-master node list
k8s_master_nodes=(
    "k8s-master01"
    "k8s-master02"
    "k8s-master03"
)

k8s_master_cert_dirs=(
    "k8s-ca"
    "k8s-apiserver"
    "k8s-admin"
    "k8s-controller-manager"
    "k8s-front-proxy-ca"
    "k8s-front-proxy-client"
    "k8s-scheduler"
    "k8s-service-account"
)

k8s_master_certs_target_dir="/etc/kubernetes/pki"

# Define k8s-worker node list
k8s_worker_nodes=(
    "k8s-worker01"
    "k8s-worker02"
    "k8s-worker03"
)

# Copy files to k8s-master nodes
for node in "${k8s_master_nodes[@]}"; do
    for cert_dir in "${k8s_master_cert_dirs[@]}"; do
        ssh root@$node "mkdir $k8s_master_certs_target_dir/$cert_dir"
	    scp ./$cert_dir/* $node:$k8s_master_certs_target_dir/$cert_dir
    done
    
    echo "Files copied to $node (k8s-master)"
done

# Copy files to k8s-worker nodes
# for node in "${k8s_worker_nodes[@]}"; do
#     for bin_file in "${k8s_worker_bins[@]}"; do
# 	    scp ./bin/$bin_file "$node:/usr/local/bin/"
#     done
#     
#     echo "Files copied to $node (k8s-worker)"
# done

echo "Script completed."
```



```shell
vi k8s-master-conf-copy.sh
```

```shell
#!/bin/bash

# Define k8s-master node list
k8s_master_nodes=(
    "k8s-master01"
    "k8s-master02"
    "k8s-master03"
)

k8s_master_conf_files=(
    "admin.conf"
    "controller-manager.conf"
    "scheduler.conf"
)

k8s_master_conf_target_dir="/etc/kubernetes"

# Define k8s-worker node list
k8s_worker_nodes=(
    "k8s-worker01"
    "k8s-worker02"
    "k8s-worker03"
)

# Copy files to k8s-master nodes
for node in "${k8s_master_nodes[@]}"; do
    for conf_file in "${k8s_master_conf_files[@]}"; do
	    scp ./$conf_file $node:$k8s_master_conf_target_dir/
    done
    
    echo "Files copied to $node (k8s-master)"
done

# Copy files to k8s-worker nodes
# for node in "${k8s_worker_nodes[@]}"; do
#     for bin_file in "${k8s_worker_bins[@]}"; do
# 	    scp ./bin/$bin_file "$node:/usr/local/bin/"
#     done
#     
#     echo "Files copied to $node (k8s-worker)"
# done

echo "Script completed."
```

```shell
chmod +x k8s-master-conf-copy.sh
```

```shell
./k8s-master-conf-copy.sh
```



## 

## 14 配置 apiserver

https://kubernetes.io/zh-cn/docs/reference/command-line-tools-reference/kube-apiserver/

1 配置

所有Master节点创建 kube-apiserver.service ，  

注意，如果不是高可用集群，192.168.0.250改为master01的地址

以下文档使用的 k8s service 网段为 10.96.0.0/16 ，该网段不能和宿主机的网段、Pod网段的重复

特别注意：docker的网桥默认为 172.17.0.1/16 。不要使用这个网段



每个master节点都需要执行以下内容  

--advertise-address： 需要改为本master节点的ip  

--service-cluster-ip-range=10.96.0.0/16： 需要改为自己规划的service网段  

--etcd-servers： 改为自己etcd-server的所有地址  

```shell
vi /usr/lib/systemd/system/kube-apiserver.service
```

通常使用的 service 配置文件如下:

```
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-apiserver \
    # 日志级别详细程度的数字
    --v=2 \
    
    --allow-privileged=true \
    
    # 默认值："0.0.0.0" 用来监听 --secure-port 端口的 IP 地址。集群的其余部分以及 CLI/web 客户端必须可以访问所关联的接口。
    # 如果为空白或未指定地址（0.0.0.0 或 ::），则将使用所有接口。
    --bind-address=0.0.0.0 \
    
    # 默认值：6443 带身份验证和鉴权机制的 HTTPS 服务端口。不能用 0 关闭。
    --secure-port=6443 \
    
    # 参数已删除
    --insecure-port=0 \
    
    # 向集群成员通知 apiserver 消息的 IP 地址。这个地址必须能够被集群中其他成员访问。
    # 如果 IP 地址为空，将会使用 --bind-address， 如果未指定 --bind-address，将会使用主机的默认接口地址。
    --advertise-address=192.168.0.151 \
    
    # CIDR 表示的 IP 范围用来为服务分配集群 IP。此地址不得与指定给节点或 Pod 的任何 IP 范围重叠
    --service-cluster-ip-range=10.96.0.0/16 \
    
    # 保留给具有 NodePort 可见性的服务的端口范围。例如："30000-32767"。范围的两端都包括在内。
    --service-node-port-range=30000-32767 \
    
    # 要连接的 etcd 服务器列表（scheme://ip:port），以逗号分隔。
    --etcdservers=https://192.168.0.151:2379,https://192.168.0.152:2379,https://192.168.0.153:2379 \
    
    # 用于保护 etcd 通信的 SSL 证书颁发机构文件。
    --etcd-ca-file=/etc/kubernetes/pki/etcd-ca/etcd-ca.pem \
    
    # 用于保护 etcd 通信的 SSL 证书文件。
    --etcd-cert-file=/etc/kubernetes/pki/etcd-client/etcd-client.pem \
    
    # 用于保护 etcd 通信的 SSL 密钥文件。
    --etcd-keyfile=/etc/kubernetes/pki/etcd-client/etcd-client-key.pem \
    
    # 如果已设置，则使用与客户端证书的 CommonName 对应的标识对任何出示由 client-ca 文件中的授权机构之一签名的客户端证书的请求进行身份验证。
    --client-ca-file=/etc/kubernetes/pki/k8s-ca/k8s-ca.pem \
    
    # 包含用于 HTTPS 的默认 x509 证书的文件。（CA 证书（如果有）在服务器证书之后并置）。
    # 如果启用了 HTTPS 服务，并且未提供 --tls-cert-file 和 --tls-private-key-file， 
    # 为公共地址生成一个自签名证书和密钥，并将其保存到 --cert-dir 指定的目录中。
    --tls-cert-file=/etc/kubernetes/pki/k8s-apiserver/k8s-apiserver.pem \
    
    # 包含匹配 --tls-cert-file 的 x509 证书私钥的文件。
    --tls-private-key-file=/etc/kubernetes/pki/k8s-apiserver/k8s-apiserver-key.pem \
    
    # TLS 的客户端证书文件的路径。
    --kubelet-client-certificate=/etc/kubernetes/pki/k8s-apiserver/k8s-apiserver.pem \
    
    # TLS 客户端密钥文件的路径。
    --kubelet-client-key=/etc/kubernetes/pki/k8s-apiserver/k8s-apiserver-key.pem \
    
    # 包含 PEM 编码的 x509 RSA 或 ECDSA 私钥或公钥的文件，用于验证 ServiceAccount 令牌。指定的文件可以包含多个键，
    # 并且可以使用不同的文件多次指定标志。如果未指定，则使用 --tls-private-key-file。提供 --service-account-signing-key 时必须指定。
    --service-account-key-file=/etc/kubernetes/pki/k8s-service-account/k8s-service-account.pub \
    
    # 包含服务帐户令牌颁发者当前私钥的文件的路径。颁发者将使用此私钥签署所颁发的 ID 令牌。
    --service-account-signing-key-file=/etc/kubernetes/pki/k8s-service-account/k8s-service-account.key \
    
    # 服务帐号令牌颁发者的标识符。颁发者将在已颁发令牌的 "iss" 声明中检查此标识符。此值为字符串或 URI。
    # 如果根据 OpenID Discovery 1.0 规范检查此选项不是有效的 URI，则即使特性门控设置为 true， 
    # ServiceAccountIssuerDiscovery 功能也将保持禁用状态。
    # 强烈建议该值符合 OpenID 规范：https://openid.net/specs/openid-connect-discovery-1_0.html。
    # 实践中，这意味着 service-account-issuer 取值必须是 HTTPS URL。
    # 还强烈建议此 URL 能够在 {service-account-issuer}/.well-known/openid-configuration 处提供 OpenID 发现文档。
    # 当此值被多次指定时，第一次的值用于生成令牌，所有的值用于确定接受哪些发行人。
    --service-account-issuer=https://kubernetes.default.svc.cluster.local \
    
    # 默认值：Hostname,InternalDNS,InternalIP,ExternalDNS,ExternalIP 用于 kubelet 连接的首选 NodeAddressTypes 列表。
    --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \
    
    # 除了默认启用的插件（NamespaceLifecycle、LimitRanger、ServiceAccount、TaintNodesByCondition、PodSecurity、
    # Priority、DefaultTolerationSeconds、DefaultStorageClass、StorageObjectInUseProtection、PersistentVolumeClaimResize、
    # RuntimeClass、CertificateApproval、CertificateSigning、CertificateSubjectRestriction、DefaultIngressClass、
    # MutatingAdmissionWebhook、ValidatingAdmissionWebhook、ResourceQuota）
    # 之外要启用的插件, 取值为逗号分隔的准入插件列表：
    # AlwaysAdmit、AlwaysDeny、AlwaysPullImages、CertificateApproval、CertificateSigning、CertificateSubjectRestriction、
    # DefaultIngressClass、DefaultStorageClass、DefaultTolerationSeconds、DenyServiceExternalIPs、EventRateLimit、
    # ExtendedResourceToleration、ImagePolicyWebhook、LimitPodHardAntiAffinityTopology、LimitRanger、MutatingAdmissionWebhook、
    # NamespaceAutoProvision、NamespaceExists、NamespaceLifecycle、NodeRestriction、OwnerReferencesPermissionEnforcement、
    # PersistentVolumeClaimResize、PersistentVolumeLabel、PodNodeSelector、PodSecurity、PodSecurityPolicy、
    # PodTolerationRestriction、Priority、ResourceQuota、RuntimeClass、SecurityContextDeny、ServiceAccount、
    # StorageObjectInUseProtection、TaintNodesByCondition、ValidatingAdmissionWebhook
    # 该标志中插件的顺序无关紧要。
    --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota \
    
    # 默认值："AlwaysAllow" 在安全端口上进行鉴权的插件的顺序列表。逗号分隔的列表：AlwaysAllow、AlwaysDeny、ABAC、Webhook、RBAC、Node。
    --authorization-mode=Node,RBAC \
    
    # 启用以允许将 "kube-system" 名字空间中类型为 "bootstrap.kubernetes.io/token" 的 Secret 用于 TLS 引导身份验证。
    --enable-bootstrap-token-auth=true \
    
    # 在信任请求头中以 --requestheader-username-headers 指示的用户名之前， 用于验证接入请求中客户端证书的根证书包。
    # 警告：一般不要假定传入请求已被授权。
    --requestheader-client-ca-file=/etc/kubernetes/pki/k8s-front-proxy-ca/k8s-front-proxy-ca.pem \
    
    # 当必须调用外部程序以处理请求时，用于证明聚合器或者 kube-apiserver 的身份的客户端证书。包括代理转发到用户 api-server 的请求
    # 和调用 Webhook 准入控制插件的请求。Kubernetes 期望此证书包含来自于 --requestheader-client-ca-file 标志中所给 CA 的签名。
    # 该 CA 在 kube-system 命名空间的 "extension-apiserver-authentication" ConfigMap 中公开。从 kube-aggregator 收到调用的组件
    # 应该使用该 CA 进行各自的双向 TLS 验证。
    --proxy-client-cert-file=/etc/kubernetes/pki/k8s-front-proxy-client/k8s-front-proxy-client.pem \
    
    # 当必须调用外部程序来处理请求时，用来证明聚合器或者 kube-apiserver 的身份的客户端私钥。这包括代理转发给用户 api-server 的请求
    # 和调用 Webhook 准入控制插件的请求。
    --proxy-client-key-file=/etc/kubernetes/pki/k8s-front-proxy-client/k8s-front-proxy-client-key.pem \
    
    # 此值为客户端证书通用名称（Common Name）的列表；表中所列的表项可以用来提供用户名， 方式是使用 --requestheader-username-headers 所指定
    # 的头部。如果为空，能够通过 --requestheader-client-ca-file 中机构 认证的客户端证书都是被允许的。
    --requestheader-allowed-names=aggregator,front-proxy-client \
    
    # 用于查验用户组的请求头部列表。建议使用 X-Remote-Group。
    --requestheader-group-headers=X-Remote-Group \
    
    # 用于查验请求头部的前缀列表。建议使用 X-Remote-Extra-。
    --requestheader-extra-headers-prefix=X-Remote-Extra- \
    
    # 用于查验用户名的请求头头列表。建议使用 X-Remote-User
    --requestheader-username-headers=X-Remote-User
    
    # 如果设置该值，这个文件将被用于通过令牌认证来保护 API 服务的安全端口。
	# --token-auth-file=/etc/kubernetes/token.csv
	
Restart=on-failure
RestartSec=10s
LimitNOFILE=65535

[Install]
WantedBy=multi-user.target
```

删除注释版本:

```toml
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-apiserver \
    --v=2 \
    --allow-privileged=true \
    --bind-address=0.0.0.0 \
    --secure-port=6443 \
    --advertise-address=192.168.0.151 \
    --service-cluster-ip-range=10.96.0.0/16 \
    --service-node-port-range=30000-32767 \
    --etcd-servers=https://192.168.0.151:2379,https://192.168.0.152:2379,https://192.168.0.153:2379 \
    --etcd-cafile=/etc/kubernetes/pki/etcd-ca/etcd-ca.pem \
    --etcd-certfile=/etc/kubernetes/pki/etcd-client/etcd-client.pem \
    --etcd-keyfile=/etc/kubernetes/pki/etcd-client/etcd-client-key.pem \
    --client-ca-file=/etc/kubernetes/pki/k8s-ca/k8s-ca.pem \
    --tls-cert-file=/etc/kubernetes/pki/k8s-apiserver/k8s-apiserver.pem \
    --tls-private-key-file=/etc/kubernetes/pki/k8s-apiserver/k8s-apiserver-key.pem \
    --kubelet-client-certificate=/etc/kubernetes/pki/k8s-apiserver/k8s-apiserver.pem \
    --kubelet-client-key=/etc/kubernetes/pki/k8s-apiserver/k8s-apiserver-key.pem \
    --service-account-key-file=/etc/kubernetes/pki/k8s-service-account/k8s-service-account.pub \
    --service-account-signing-key-file=/etc/kubernetes/pki/k8s-service-account/k8s-service-account.key \
    --service-account-issuer=https://kubernetes.default.svc.cluster.local \
    --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \
    --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota \
    --authorization-mode=Node,RBAC \
    --enable-bootstrap-token-auth=true \
    --requestheader-client-ca-file=/etc/kubernetes/pki/k8s-front-proxy-ca/k8s-front-proxy-ca.pem \
    --proxy-client-cert-file=/etc/kubernetes/pki/k8s-front-proxy-client/k8s-front-proxy-client.pem \
    --proxy-client-key-file=/etc/kubernetes/pki/k8s-front-proxy-client/k8s-front-proxy-client-key.pem \
    --requestheader-allowed-names=aggregator,front-proxy-client \
    --requestheader-group-headers=X-Remote-Group \
    --requestheader-extra-headers-prefix=X-Remote-Extra- \
    --requestheader-username-headers=X-Remote-User
	# --token-auth-file=/etc/kubernetes/token.csv
	
Restart=on-failure
RestartSec=10s
LimitNOFILE=65535

[Install]
WantedBy=multi-user.target
```



通过脚本生成各个节点的service文件

```shell
vi kube-apiserver-service-gen.sh
```

```shell
#!/bin/bash

# Define etcd node list with names and IP addresses
etcd_nodes=(
    "192.168.0.151"
    "192.168.0.152"
    "192.168.0.153"
)

# Generate etcdservers value
etcdservers=""
for etcd_node in "${etcd_nodes[@]}"; do
    if [ -n "$etcdservers" ]; then
        etcdservers+=","
    fi
    etcdservers+="https://$etcd_node:2379"
done


# Define k8s-master node list with names and IP addresses
k8s_master_nodes=(
    "k8s-master01|192.168.0.151"
    "k8s-master02|192.168.0.152"
    "k8s-master03|192.168.0.153"
)

mkdir kube-apiserver-service

# Loop through each k8s-master node
for node in "${k8s_master_nodes[@]}"; do
    IFS='|' read -ra node_info <<< "$node"
    node_name="${node_info[0]}"
    node_ip="${node_info[1]}"
    
    # Generate configuration content
    config_content="[Unit]\n"
    config_content+="Description=Kubernetes API Server\n"
    config_content+="Documentation=https://github.com/kubernetes/kubernetes\n"
    config_content+="After=network.target\n\n"
    
    config_content+="[Service]\n"
    config_content+="ExecStart=/usr/local/bin/kube-apiserver \\\\\n"
    config_content+="    --v=2 \\\\\n"
    config_content+="    --allow-privileged=true \\\\\n"
    config_content+="    --bind-address=0.0.0.0 \\\\\n"
    config_content+="    --secure-port=6443 \\\\\n"
    config_content+="    --advertise-address=$node_ip \\\\\n"
    config_content+="    --service-cluster-ip-range=10.96.0.0/16 \\\\\n"
    config_content+="    --service-node-port-range=30000-32767 \\\\\n"
    config_content+="    --etcd-servers=$etcdservers \\\\\n"
    config_content+="    --etcd-cafile=/etc/kubernetes/pki/etcd-ca/etcd-ca.pem \\\\\n"
    config_content+="    --etcd-certfile=/etc/kubernetes/pki/etcd-client/etcd-client.pem \\\\\n"
    config_content+="    --etcd-keyfile=/etc/kubernetes/pki/etcd-client/etcd-client-key.pem \\\\\n"
    config_content+="    --client-ca-file=/etc/kubernetes/pki/k8s-ca/k8s-ca.pem \\\\\n"
    config_content+="    --tls-cert-file=/etc/kubernetes/pki/k8s-apiserver/k8s-apiserver.pem \\\\\n"
    config_content+="    --tls-private-key-file=/etc/kubernetes/pki/k8s-apiserver/k8s-apiserver-key.pem \\\\\n"
    config_content+="    --kubelet-client-certificate=/etc/kubernetes/pki/k8s-apiserver/k8s-apiserver.pem \\\\\n"
    config_content+="    --kubelet-client-key=/etc/kubernetes/pki/k8s-apiserver/k8s-apiserver-key.pem \\\\\n"
    config_content+="    --service-account-key-file=/etc/kubernetes/pki/k8s-service-account/k8s-service-account.pub \\\\\n"
    config_content+="    --service-account-signing-key-file=/etc/kubernetes/pki/k8s-service-account/k8s-service-account.key \\\\\n"
    config_content+="    --service-account-issuer=https://kubernetes.default.svc.cluster.local \\\\\n"
    config_content+="    --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \\\\\n"
    config_content+="    --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota \\\\\n"
    config_content+="    --authorization-mode=Node,RBAC \\\\\n"
    config_content+="    --enable-bootstrap-token-auth=true \\\\\n"
    config_content+="    --requestheader-client-ca-file=/etc/kubernetes/pki/k8s-front-proxy-ca/k8s-front-proxy-ca.pem \\\\\n"
    config_content+="    --proxy-client-cert-file=/etc/kubernetes/pki/k8s-front-proxy-client/k8s-front-proxy-client.pem \\\\\n"
    config_content+="    --proxy-client-key-file=/etc/kubernetes/pki/k8s-front-proxy-client/k8s-front-proxy-client-key.pem \\\\\n"
    config_content+="    --requestheader-allowed-names=aggregator,front-proxy-client \\\\\n"
    config_content+="    --requestheader-group-headers=X-Remote-Group \\\\\n"
    config_content+="    --requestheader-extra-headers-prefix=X-Remote-Extra- \\\\\n"
    config_content+="    --requestheader-username-headers=X-Remote-User \\\\\n"
    config_content+="    # --token-auth-file=/etc/kubernetes/token.csv\n\n"
    
    config_content+="Restart=on-failure\n"
    config_content+="RestartSec=10s\n"
    config_content+="LimitNOFILE=65535\n\n"
    
    config_content+="[Install]\n"
    config_content+="WantedBy=multi-user.target\n"
    
    # Create configuration file
    config_file="kube-apiserver-service/$node_name-kube-apiserver.service"
    echo -e "$config_content" | sudo tee "$config_file" > /dev/null
        
    echo "Configuration file '$config_file' created for $node_name"
done

echo "Script completed."
```

将生成的配置文件分发到各个master节点

```shell
vi service-copy.sh
```

```shell
#!/bin/bash

# Define k8s-master node list
k8s_master_nodes=(
    "k8s-master01"
    "k8s-master02"
    "k8s-master03"
)

k8s_master_services=(
    "kube-apiserver"
)

k8s_master_service_target_dir="/usr/lib/systemd/system"

# Define k8s-worker node list
k8s_worker_nodes=(
    "k8s-worker01"
    "k8s-worker02"
    "k8s-worker03"
)

# Copy files to k8s-master nodes
for node in "${k8s_master_nodes[@]}"; do
    for k8s_master_service in "${k8s_master_services[@]}"; do
	    scp ./"$k8s_master_service"-service/"$node"-"$k8s_master_service".service $node:$k8s_master_service_target_dir/"$k8s_master_service".service
    done
    
    echo "Files copied to $node (k8s-master)"
done

# Copy files to k8s-worker nodes
# for node in "${k8s_worker_nodes[@]}"; do
#     for bin_file in "${k8s_worker_bins[@]}"; do
# 	    scp ./bin/$bin_file "$node:/usr/local/bin/"
#     done
#     
#     echo "Files copied to $node (k8s-worker)"
# done

echo "Script completed."
```



2、启动apiserver服务  

所有master节点执行:

```shell
systemctl daemon-reload && systemctl enable --now kube-apiserver
```

查看状态

```shell
systemctl status kube-apiserver
```



查看日志

```shell
journalctl -f -u kube-apiserver
```

重启

```shell
systemctl daemon-reload && systemctl restart kube-apiserver
```





## 15 配置controller-manager

https://kubernetes.io/zh-cn/docs/reference/command-line-tools-reference/kube-controller-manager/



1 配置

所有Master节点配置kube-controller-manager.service

文档使用的k8s Pod网段为 196.16.0.0/16 ，该网段不能和宿主机的网段、k8s Service网段的重复，请按需修改;

特别注意：docker的网桥默认为 172.17.0.1/16 。不要使用这个网段

在本地创建配置文件再分发到各个master节点

```shell
mkdir -p kube-controller-manager-service
```

```shell
vi kube-controller-manager-service/kube-controller-manager.service
```

```toml
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes
After=network.target
[Service]
ExecStart=/usr/local/bin/kube-controller-manager \
    --v=2 \
    --bind-address=0.0.0.0 \
    --root-ca-file=/etc/kubernetes/pki/k8s-ca/k8s-ca.pem \
    --cluster-signing-cert-file=/etc/kubernetes/pki/k8s-ca/k8s-ca.pem \
    --cluster-signing-key-file=/etc/kubernetes/pki/k8s-ca/k8s-ca-key.pem \
    --service-account-private-key-file=/etc/kubernetes/pki/k8s-service-account/k8s-service-account.key \
    --kubeconfig=/etc/kubernetes/controller-manager.conf \
    --leader-elect=true \
    --use-service-account-credentials=true \
    --node-monitor-grace-period=40s \
    --node-monitor-period=5s \
    --controllers=*,bootstrapsigner,tokencleaner \
    --allocate-node-cidrs=true \
    --service-cluster-ip-range=10.96.0.0/16 \
    --cluster-cidr=196.16.0.0/16 \
    --node-cidr-mask-size-ipv4=24 \
    --requestheader-client-ca-file=/etc/kubernetes/pki/k8s-front-proxy-ca/k8s-front-proxy-ca.pem
    
Restart=always
RestartSec=10s

[Install]
WantedBy=multi-user.target
```

```shell
vi kube-controller-manager-service-copy.sh
```

```shell
#!/bin/bash

# Define k8s-master node list
k8s_master_nodes=(
    "k8s-master01"
    "k8s-master02"
    "k8s-master03"
)

k8s_master_services=(
    "kube-controller-manager"
)

k8s_master_service_target_dir="/usr/lib/systemd/system"

# Define k8s-worker node list
k8s_worker_nodes=(
    "k8s-worker01"
    "k8s-worker02"
    "k8s-worker03"
)

# Copy files to k8s-master nodes
for node in "${k8s_master_nodes[@]}"; do
    for k8s_master_service in "${k8s_master_services[@]}"; do
            scp ./"$k8s_master_service"-service/"$k8s_master_service".service $node:$k8s_master_service_target_dir/"$k8s_master_service".service
    done

    echo "Files copied to $node (k8s-master)"
done

# Copy files to k8s-worker nodes
# for node in "${k8s_worker_nodes[@]}"; do
#     for bin_file in "${k8s_worker_bins[@]}"; do
#           scp ./bin/$bin_file "$node:/usr/local/bin/"
#     done
#
#     echo "Files copied to $node (k8s-worker)"
# done

echo "Script completed."
```

```shell
chmod +x kube-controller-manager-service-copy.sh
```

```shell
./kube-controller-manager-service-copy.sh
```



2、启动

所有master节点执行

```shell
systemctl daemon-reload && systemctl enable --now kube-controller-manager
```

```shell
systemctl status kube-controller-manager
```



查看日志

```shell
journalctl -f -u kube-controller-manager
```

重启

```shell
systemctl daemon-reload && systemctl restart kube-controller-manager
```





## 16 配置scheduler  

https://kubernetes.io/zh-cn/docs/reference/command-line-tools-reference/kube-scheduler/

1、配置

```shell
mkdir kube-scheduler-service
```

```shell
vi kube-scheduler-service/kube-scheduler.service
```

```toml
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-scheduler \
    --v=2 \
    --bind-address=0.0.0.0  \
    --leader-elect=true \
    --kubeconfig=/etc/kubernetes/scheduler.conf

Restart=always
RestartSec=10s

[Install]
WantedBy=multi-user.target
```



```shell
vi kube-scheduler-service-copy.sh
```

```
#!/bin/bash

# Define k8s-master node list
k8s_master_nodes=(
    "k8s-master01"
    "k8s-master02"
    "k8s-master03"
)

k8s_master_services=(
    "kube-scheduler"
)

k8s_master_service_target_dir="/usr/lib/systemd/system"

# Define k8s-worker node list
k8s_worker_nodes=(
    "k8s-worker01"
    "k8s-worker02"
    "k8s-worker03"
)

# Copy files to k8s-master nodes
for node in "${k8s_master_nodes[@]}"; do
    for k8s_master_service in "${k8s_master_services[@]}"; do
            scp ./"$k8s_master_service"-service/"$k8s_master_service".service $node:$k8s_master_service_target_dir/"$k8s_master_service".service
    done

    echo "Files copied to $node (k8s-master)"
done

# Copy files to k8s-worker nodes
# for node in "${k8s_worker_nodes[@]}"; do
#     for bin_file in "${k8s_worker_bins[@]}"; do
#           scp ./bin/$bin_file "$node:/usr/local/bin/"
#     done
#
#     echo "Files copied to $node (k8s-worker)"
# done

echo "Script completed."
```

```
chmod +x kube-scheduler-service-copy.sh
```

```
./kube-scheduler-service-copy.sh
```



2、启动  

```shell
systemctl daemon-reload
systemctl daemon-reload && systemctl enable --now kube-scheduler
systemctl status kube-scheduler
```



## 17 配置 kubectl

TLS与引导启动原理  

TLS Bootstrapping原理参照:

https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/kubelet-tls-bootstrapping/



### 1 配置bootstrap

注意，如果不是高可用集群， 192.168.0.250:6443 改为master1的地址，6443为apiserver的默认端口  

准备一个随机token。但是我们只需要16个字符



命令:

```shell
head -c 16 /dev/urandom | od -An -t x | tr -d ' '
```

值如下： ed8324d10ded49e12cb5517ee0a2e1a6



生成16个字符的

```shell
head -c 8 /dev/urandom | od -An -t x | tr -d ' '
```

ce89617226b35719



设置集群

```shell
kubectl config set-cluster kubernetes \
--certificate-authority=/etc/kubernetes/pki/k8s-ca/k8s-ca.pem \
--embed-certs=true \
--server=https://192.168.0.250:6443 \
--kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf
```



设置秘钥

```shell
kubectl config set-credentials tls-bootstrap-token-user \
--token=g6kw4b.ce89617226b35719 \
--kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf
```



设置上下文

```shell
kubectl config set-context tls-bootstrap-token-user@kubernetes \
--cluster=kubernetes \
--user=tls-bootstrap-token-user \
--kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf
```



使用设置

```shell
kubectl config use-context tls-bootstrap-token-user@kubernetes \
--kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf
```



可以在本地生成配置



```shell
kubectl config set-cluster kubernetes \
--certificate-authority=k8s-ca/k8s-ca.pem \
--embed-certs=true \
--server=https://192.168.0.250:6443 \
--kubeconfig=bootstrap-kubelet.conf

kubectl config set-credentials tls-bootstrap-token-user \
--token=g6kw4b.ce89617226b35719 \
--kubeconfig=bootstrap-kubelet.conf

kubectl config set-context tls-bootstrap-token-user@kubernetes \
--cluster=kubernetes \
--user=tls-bootstrap-token-user \
--kubeconfig=bootstrap-kubelet.conf

kubectl config use-context tls-bootstrap-token-user@kubernetes \
--kubeconfig=bootstrap-kubelet.conf
```

再将配置文件分发到控制节点, 一般只在 k8s-master01 上操作集群, 因此默认仅将配置文件复制到 k8s-master01 节点, 也可以复制到多个节点, 防止 k8s-master01 节点异常时无法操作集群

```shell
vi bootstrap-kubelet-conf-copy.sh
```

```shell
#!/bin/bash

nodes=(
    "k8s-master01"
    "k8s-master02"
    "k8s-master03"
)

target_directory="/etc/kubernetes"

files=(
    "bootstrap-kubelet.conf"
)

for node in "${nodes[@]}"; do
    for file in $files; do
	    echo copying $file to $node  $target_directory
    	scp $file $node:"$target_directory"/;
    done
    
    echo "Copying to $node completed."
done

echo "Script completed."
```

```shell
chmod +x bootstrap-kubelet-conf-copy.sh
```

```shell
./bootstrap-kubelet-conf-copy.sh
```



### 2 配置 kubectl 执行权限  

kubectl 能不能操作集群是看 /root/.kube 下有没有config文件，而config就是我们之前生成的admin.conf，具有操作权限的  

只在master1生成，因为生产集群，我们只能让一台机器具有操作集群的权限，这样好控制

在 k8s-master01 执行:

```shell
mkdir -p /root/.kube
cp /etc/kubernetes/admin.conf /root/.kube/config
```

可以在本机执行:

```shell
vi admin-conf-copy.sh
```

```shell
#!/bin/bash

nodes=(
    "k8s-master01"
    "k8s-master02"
    "k8s-master03"
)

target_directory="/root/.kube"

files=(
    "admin.conf|config"
)

for node in "${nodes[@]}"; do
	ssh root@$node "mkdir -p $target_directory"
    for file in $files; do
        IFS='|' read -ra file_info <<< "$file"
        src_file_name="${file_info[0]}"
        target_file_name="${file_info[1]}"
	    echo copying $src_file_name to $node  $target_directory/$target_file_name
    	scp $src_file_name $node:"$target_directory"/"$target_file_name";
    done
    
    echo "Copying to $node completed."
done

echo "Script completed."
```

```shell
chmod +x admin-conf-copy.sh
```

```shell
./admin-conf-copy.sh
```



验证

```shell
kubectl get nodes
```

应该在网络里面开放负载均衡器的6443端口;默认应该不要配置的

```shell
kubectl get nodes
```

```shell
No resources found
```

说明已经可以连接apiserver并获取资源



### 3 创建集群引导权限文件  

在本机创建这个文件

```shell
vi /etc/kubernetes/bootstrap.secret.yaml
```

也可以在本机创建这个文件

```shell
vi /etc/kubernetes/bootstrap.secret.yaml
```

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: bootstrap-token-g6kw4b
  namespace: kube-system
type: bootstrap.kubernetes.io/token
stringData:
  description: "The default bootstrap token generated by 'kubelet '."
  token-id: g6kw4b
  token-secret: ce89617226b35719
  usage-bootstrap-authentication: "true"
  usage-bootstrap-signing: "true"
  auth-extra-groups: system:bootstrappers:default-nodetoken,system:bootstrappers:worker,system:bootstrappers:ingress

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kubelet-bootstrap
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:node-bootstrapper
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:bootstrappers:default-node-token

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: node-autoapprove-bootstrap
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:certificates.k8s.io:certificatesigningrequests:nodeclient
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:bootstrappers:default-node-token
    
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: node-autoapprove-certificate-rotation
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:nodes
  
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kube-apiserver-to-kubelet
  
rules:
  - apiGroups:
    - ""
    resources:
      - nodes/proxy
      - nodes/stats
      - nodes/log
      - nodes/spec
      - nodes/metrics
    verbs:
      - "*"
      
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:kube-apiserver
  namespace: ""
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kube-apiserver-to-kubelet
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: kube-apiserver
```



应用此文件资源内容

```shell
kubectl create -f /etc/kubernetes/bootstrap.secret.yaml
```





## 16 配置 kubelet

https://kubernetes.io/zh-cn/docs/reference/command-line-tools-reference/kubelet/

https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/



1、创建 kubelet.service  

所有节点，配置kubelet服务

```shell
vi /usr/lib/systemd/system/kubelet.service
```

也可以在本地创建

```shell
vi kubelet.service
```

```toml
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=containerd.service
Requires=containerd.service

[Service]
ExecStart=/usr/local/bin/kubelet \
--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf \
--kubeconfig=/etc/kubernetes/kubelet.conf \
--config=/etc/kubernetes/kubelet-conf.yml \
--container-runtime-endpoint=unix:///run/containerd/containerd.sock \
--node-labels=node.kubernetes.io/node=''

Restart=always
StartLimitInterval=0
RestartSec=10

[Install]
WantedBy=multi-user.target
```

--bootstrap-kubeconfig : 某 kubeconfig 文件的路径，该文件将用于获取 kubelet 的客户端证书。 如果 `--kubeconfig` 所指定的文件不存在，则使用引导所用 kubeconfig 从 API 服务器请求客户端证书。成功后，将引用生成的客户端证书和密钥的 kubeconfig 写入 --kubeconfig 所指定的路径。客户端证书和密钥文件将存储在 `--cert-dir` 所指的目录。

--kubeconfig: 只需要配置一个路径即可, 文件由kubelet自动生成





2 创建 kubelet-conf.yml 文件  

```shell
vi /etc/kubernetes/kubelet-conf.yml
```

在本机创建, 然后再分发到所有节点

```shell
vi kubelet-conf.yml
```

clusterDNS 为service网络的第10个ip值,改成自己的。如：10.96.0.10

clientCAFile : 修改为 k8s的 CA 证书路径

staticPodPath: 静态 pod 的配置路径, 一般设置为 /etc/kubernetes/manifests

```shell
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
address: 0.0.0.0
port: 10250
readOnlyPort: 10255
authentication:
    anonymous:
        enabled: false
    webhook:
        cacheTTL: 2m0s
        enabled: true
    x509:
        clientCAFile: /etc/kubernetes/pki/k8s-ca/k8s-ca.pem
authorization:
    mode: Webhook
    webhook:
        cacheAuthorizedTTL: 5m0s
        cacheUnauthorizedTTL: 30s
cgroupDriver: systemd
cgroupsPerQOS: true
clusterDNS:
- 10.96.0.10
clusterDomain: cluster.local
containerLogMaxFiles: 5
containerLogMaxSize: 10Mi
contentType: application/vnd.kubernetes.protobuf
cpuCFSQuota: true
cpuManagerPolicy: none
cpuManagerReconcilePeriod: 10s
enableControllerAttachDetach: true
enableDebuggingHandlers: true
enforceNodeAllocatable:
- pods
eventBurst: 10
eventRecordQPS: 5
evictionHard:
    imagefs.available: 15%
    memory.available: 100Mi
    nodefs.available: 10%
    nodefs.inodesFree: 5%
evictionPressureTransitionPeriod: 5m0s
failSwapOn: true
fileCheckFrequency: 20s
hairpinMode: promiscuous-bridge
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 20s
imageGCHighThresholdPercent: 85
imageGCLowThresholdPercent: 80
imageMinimumGCAge: 2m0s
iptablesDropBit: 15
iptablesMasqueradeBit: 14
kubeAPIBurst: 10
kubeAPIQPS: 5
makeIPTablesUtilChains: true
maxOpenFiles: 1000000
maxPods: 110
nodeStatusUpdateFrequency: 10s
oomScoreAdj: -999
podPidsLimit: -1
registryBurst: 10
registryPullQPS: 5
resolvConf: /etc/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 2m0s
serializeImagePulls: true
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 4h0m0s
syncFrequency: 1m0s
volumeStatsAggPeriod: 1m0s
```



3 将各个配置文件发送到相应的节点

所有 worker 节点创建相关目录

```shell
vi k8s-kubelet-set.sh
```

所有node节点必须有 kubelet kube-proxy 可执行文件

```shell
#!/bin/bash

master_nodes=(
	"k8s-master01"
    "k8s-master02"
    "k8s-master03"
)

worker_nodes=(
    "k8s-worker01"
    "k8s-worker02"
    "k8s-worker03"
)

nodes=("${master_nodes[@]}" "${worker_nodes[@]}")

target_directory="/root/.kube"

# 1 在 worker 节点中创建目录
dirs=(
    "/var/lib/kubelet"
    "/var/log/kubernetes"
    "/etc/systemd/system/kubelet.service.d"
    "/etc/kubernetes/manifests/"
)

#dirs_string=/var/lib/kubelet /var/log/kubernetes ...
dirs_string="${dirs[*]}"

for node in "${worker_nodes[@]}"; do
	ssh root@$node "mkdir -p $dirs_string" 
    echo "make dirs for $node completed."
done


# 2 将 kubelet.service 发送到所有节点, 保存为 /usr/lib/systemd/system/kubelet.service
# 3 将 bootstrap-kubelet.conf 发送到所有的节点, 保存为 /etc/kubernetes/bootstrap-kubelet.conf
# 4 将 kubelet-conf.yml 发送到所有节点, 保存为 /etc/kubernetes/kubelet-conf.yml

files=(
    "kubelet.service|/usr/lib/systemd/system|kubelet.service"
    "bootstrap-kubelet.conf|/etc/kubernetes|bootstrap-kubelet.conf"
    "kubelet-conf.yml|/etc/kubernetes|kubelet-conf.yml"
)
for node in "${nodes[@]}"; do
	for file in "${files[@]}"; do
		IFS='|' read -ra file_info <<< "$file"
        src_file_name="${file_info[0]}"
        target_dir="${file_info[1]}"
        target_file_name="${file_info[2]}"
        
        ssh root@$node "[ -d \"$target_dir\" ] || mkdir -p \"$target_dir\""
        
        echo copying $src_file_name to $node  save as "$target_dir"/"$target_file_name"
    	scp $src_file_name $node:"$target_dir"/"$target_file_name";
	done
done

# 5 将 k8s-ca/k8s-ca.pem 发送到 worker 节点( master节点已经保存了 ),  保存为 /etc/kubernetes/pki/k8s-ca/k8s-ca.pem
worker_files=(
    "k8s-ca/k8s-ca.pem|/etc/kubernetes/pki/k8s-ca|k8s-ca.pem"
)
for node in "${worker_nodes[@]}"; do
	for file in "${worker_files[@]}"; do
		IFS='|' read -ra file_info <<< "$file"
        src_file_name="${file_info[0]}"
        target_dir="${file_info[1]}"
        target_file_name="${file_info[2]}"
        
        ssh root@$node "[ -d \"$target_dir\" ] || mkdir -p \"$target_dir\""

        echo copying $src_file_name to $node  save as "$target_dir"/"$target_file_name"
    	scp $src_file_name $node:"$target_dir"/"$target_file_name";
	done
done

echo "Script completed."
```

```shell
chmod +x k8s-kubelet-set.sh
```

```shell
./k8s-kubelet-set.sh
```



3 所有节点启动kubelet  

```
systemctl daemon-reload && systemctl enable --now kubelet
systemctl status kubelet
```

会提示 "Unable to update cni config"。接下来配置cni网络即可  

可以在本机执行:

```shell
vi kubelet-start.sh
```

```shell
#!/bin/bash

master_nodes=(
	"k8s-master01"
    "k8s-master02"
    "k8s-master03"
)

worker_nodes=(
    "k8s-worker01"
    "k8s-worker02"
    "k8s-worker03"
)

nodes=("${master_nodes[@]}" "${worker_nodes[@]}")

service_name="kubelet"

for node in "${nodes[@]}"; do
    # Check the service status remotely
    ssh root@$node "systemctl is-active $service_name"

    # Capture the exit code of the previous command
    status=$?

    if [ $status -eq 0 ]; then
        # Service is active, restart it
        ssh root@$node "systemctl daemon-reload && systemctl restart $service_name"
        echo "Service '$service_name' was active and has been restarted."
    elif [ $status -eq 3 ]; then
        # Service is not active, start it
        ssh root@$node "systemctl daemon-reload && systemctl start $service_name"
        echo "Service '$service_name' was not active and has been started."
    else
        echo "Unable to determine the status of service '$service_name'."
        ssh root@$node "systemctl daemon-reload && systemctl enable --now $service_name"
    fi

done

echo "Script completed."
```

```shell
chmod +x kubelet-start.sh
```

```shell
./kubelet-start.sh
```

查看状态

```shell
systemctl status kubelet
```



查看服务日志

```shell
journalctl -f -u kubelet
```



## 17 配置 kube-proxy

注意，如果不是高可用集群， 192.168.0.250:6443 改为master1的地址，6443改为apiserver的默认端口  



创建kube-proxy证书

```shell
cat > k8s-proxy-csr.json  << EOF 
{
  "CN": "system:kube-proxy",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Beijing",
      "L": "Beijing",
      "O": "system:kube-proxy",
      "OU": "Kubernetes-manual"
    }
  ]
}
EOF
```

```shell
mkdir k8s-proxy
cfssl gencert -ca=k8s-ca/k8s-ca.pem -ca-key=k8s-ca/k8s-ca-key.pem -config=ca-config.json -profile=kubernetes k8s-proxy-csr.json | cfssljson -bare k8s-proxy/k8s-proxy
```





1、生成kube-proxy.conf  

以下操作在master1执行  

创建kube-proxy的sa  

```shell
kubectl -n kube-system create serviceaccount kube-proxy
```

创建角色绑定

```shell
kubectl create clusterrolebinding system:kube-proxy \
--clusterrole system:node-proxier \
--serviceaccount kube-system:kube-proxy
```



导出变量，方便后面使用  

```shell
SECRET=$(kubectl -n kube-system get sa/kube-proxy --output=jsonpath='{.secrets[0].name}')
JWT_TOKEN=$(kubectl -n kube-system get secret/$SECRET --output=jsonpath='{.data.token}' | base64 -d)
PKI_DIR=/etc/kubernetes/pki
K8S_DIR=/etc/kubernetes
```



生成 kube-proxy 配置

--server: 指定自己的apiserver地址或者lb地址

```shell
kubectl config set-cluster kubernetes \
--certificate-authority=/etc/kubernetes/pki/ca.pem \
--embed-certs=true \
--server=https://192.168.0.250:6443 \
--kubeconfig=${K8S_DIR}/kube-proxy.conf
```



kube-proxy秘钥设置  

```shell
kubectl config set-credentials kubernetes \
--token=${JWT_TOKEN} \
--kubeconfig=/etc/kubernetes/kube-proxy.conf
```

```shell
kubectl config set-context kubernetes \
--cluster=kubernetes \
--user=kubernetes \
--kubeconfig=/etc/kubernetes/kube-proxy.conf
```

```shell
kubectl config use-context kubernetes \
--kubeconfig=/etc/kubernetes/kube-proxy.conf
```





```shell
kubectl config set-cluster kubernetes \
  --certificate-authority=k8s-ca/k8s-ca.pem \
  --embed-certs=true \
  --server=https://192.168.0.250:6443 \
  --kubeconfig=kube-proxy.conf

kubectl config set-credentials kube-proxy \
  --client-certificate=k8s-proxy/k8s-proxy.pem \
  --client-key=k8s-proxy/k8s-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-proxy.conf

kubectl config set-context kube-proxy@kubernetes \
  --cluster=kubernetes \
  --user=kube-proxy \
  --kubeconfig=kube-proxy.conf

kubectl config use-context kube-proxy@kubernetes \
--kubeconfig=kube-proxy.conf
```

 

2 配置 kube-proxy.service

所有节点配置 kube-proxy.service 服务，一会儿设置为开机启动

```shell
vi /usr/lib/systemd/system/kube-proxy.service
```

```shell
vi kube-proxy.service
```



```shell
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-proxy \
--config=/etc/kubernetes/kube-proxy.yaml \
--v=2
Restart=always
RestartSec=10s

[Install]
WantedBy=multi-user.target
```



3、准备 kube-proxy.yaml

一定注意修改自己的Pod网段范围  

所有机器执行

```shell
vi /etc/kubernetes/kube-proxy.yaml
```

```shell
vi kube-proxy.yaml
```

kubeconfig: /etc/kubernetes/kube-proxy.conf #kube-proxy引导文件

clusterCIDR: 196.16.0.0/16 #修改为自己的Pod-CIDR

```yaml
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
bindAddress: 0.0.0.0
clientConnection:
  acceptContentTypes: ""
  burst: 10
  contentType: application/vnd.kubernetes.protobuf
  kubeconfig: /etc/kubernetes/kube-proxy.conf
  qps: 5
clusterCIDR: 196.16.0.0/16
configSyncPeriod: 15m0s
conntrack:
  max: null
  maxPerCore: 32768
  min: 131072
  tcpCloseWaitTimeout: 1h0m0s
  tcpEstablishedTimeout: 24h0m0s
enableProfiling: false
healthzBindAddress: 0.0.0.0:10256
hostnameOverride: ""
iptables:
  masqueradeAll: false
  masqueradeBit: 14
  minSyncPeriod: 0s
  syncPeriod: 30s
ipvs:
  masqueradeAll: true
  minSyncPeriod: 5s
  scheduler: "rr"
  syncPeriod: 30s
metricsBindAddress: 127.0.0.1:10249
mode: "ipvs"
nodePortAddresses: null
oomScoreAdj: -999
portRange: ""
udpIdleTimeout: 250ms
```



3 将各个配置文件发送到相应的节点

```shell
vi k8s-kube-proxy-set.sh
```

```shell
#!/bin/bash

master_nodes=(
	"k8s-master01"
    "k8s-master02"
    "k8s-master03"
)

worker_nodes=(
    "k8s-worker01"
    "k8s-worker02"
    "k8s-worker03"
)

nodes=("${master_nodes[@]}" "${worker_nodes[@]}")

# 1 将 kube-proxy.conf 发送到所有节点, 保存为 /etc/kubernetes/kube-proxy.conf
# 2 将 kube-proxy.service 发送到所有的节点, 保存为 /usr/lib/systemd/system/kube-proxy.service
# 3 将 kube-proxy.yaml 发送到所有节点, 保存为 /etc/kubernetes/kube-proxy.yaml

files=(
    "kube-proxy.conf|/etc/kubernetes|kube-proxy.conf"
    "kube-proxy.service|/usr/lib/systemd/system|kube-proxy.service"
    "kube-proxy.yaml|/etc/kubernetes|kube-proxy.yaml"
)
for node in "${nodes[@]}"; do
	for file in "${files[@]}"; do
		IFS='|' read -ra file_info <<< "$file"
        src_file_name="${file_info[0]}"
        target_dir="${file_info[1]}"
        target_file_name="${file_info[2]}"
        
        ssh root@$node "[ -d \"$target_dir\" ] || mkdir -p \"$target_dir\""
        
        echo copying $src_file_name to $node  save as "$target_dir"/"$target_file_name"
    	scp $src_file_name $node:"$target_dir"/"$target_file_name";
	done
done

echo "Script completed."
```

```shell
chmod +x k8s-kube-proxy-set.sh
```

```shell
./k8s-kube-proxy-set.sh
```



3 所有节点启动 kube-proxy

```
systemctl daemon-reload && systemctl enable --now kube-proxy
systemctl status kube-proxy
```



可以在本机执行:

```shell
vi kube-proxy-start.sh
```

```shell
#!/bin/bash

master_nodes=(
	"k8s-master01"
    "k8s-master02"
    "k8s-master03"
)

worker_nodes=(
    "k8s-worker01"
    "k8s-worker02"
    "k8s-worker03"
)

nodes=("${master_nodes[@]}" "${worker_nodes[@]}")

service_name="kube-proxy"

for node in "${nodes[@]}"; do
    # Check the service status remotely
    ssh root@$node "systemctl is-active $service_name"

    # Capture the exit code of the previous command
    status=$?

    if [ $status -eq 0 ]; then
        # Service is active, restart it
        ssh root@$node "systemctl daemon-reload && systemctl restart $service_name"
        echo "Service '$service_name' was active and has been restarted."
    elif [ $status -eq 3 ]; then
        # Service is not active, start it
        ssh root@$node "systemctl daemon-reload && systemctl start $service_name"
        echo "Service '$service_name' was not active and has been started."
    else
        echo "Unable to determine the status of service '$service_name'."
        ssh root@$node "systemctl daemon-reload && systemctl enable --now $service_name"
    fi

done

echo "Script completed."
```

```shell
chmod +x kube-proxy-start.sh
```

```shell
./kube-proxy-start.sh
```

查看状态

```shell
systemctl status kube-proxy
```



查看服务日志

```shell
journalctl -f -u kube-proxy
```





4、启动kube-proxy  

所有节点启动  

```shell
systemctl daemon-reload && systemctl enable --now kube-proxy
systemctl status kube-proxy
```





## 18 部署calico

可以参照calico私有云部署指南  

```
# 下载官网calico
curl https://docs.projectcalico.org/manifests/calico-etcd.yaml -o calico.yaml
## 把这个镜像修改成国内镜像
# 修改一些我们自定义的. 修改etcd集群地址
sed -i 's#etcd_endpoints: "http://<ETCD_IP>:<ETCD_PORT>"#etcd_endpoints:
"https://192.168.0.10:2379,https://192.168.0.11:2379,https://192.168.0.12:2379"#g
' calico.yaml
# etcd的证书内容，需要base64编码设置到yaml中
ETCD_CA=`cat /etc/kubernetes/pki/etcd/ca.pem | base64 -w 0 `
ETCD_CERT=`cat /etc/kubernetes/pki/etcd/etcd.pem | base64 -w 0 `
ETCD_KEY=`cat /etc/kubernetes/pki/etcd/etcd-key.pem | base64 -w 0 `
# 替换etcd中的证书base64编码后的内容
sed -i "s@# etcd-key: null@etcd-key: ${ETCD_KEY}@g; s@# etcd-cert: null@etcdcert: ${ETCD_CERT}@g; s@# etcd-ca: null@etcd-ca: ${ETCD_CA}@g" calico.yaml
#打开 etcd_ca 等默认设置（calico启动后自己生成）。
sed -i 's#etcd_ca: ""#etcd_ca: "/calico-secrets/etcd-ca"#g; s#etcd_cert:
""#etcd_cert: "/calico-secrets/etcd-cert"#g; s#etcd_key: "" #etcd_key: "/calicosecrets/etcd-key" #g' calico.yaml
# 修改自己的Pod网段 196.16.0.0/16
POD_SUBNET="196.16.0.0/16"
sed -i 's@# - name: CALICO_IPV4POOL_CIDR@- name: CALICO_IPV4POOL_CIDR@g; s@#
value: "192.168.0.0/16"@ value: '"${POD_SUBNET}"'@g' calico.yaml
# 一定确定自己是否修改好了
#确认calico是否修改好
grep "CALICO_IPV4POOL_CIDR" calico.yaml -A 1
```

```
# 应用calico配置
kubectl apply -f calico.yaml
```



11、部署coreDNS  

```shell
git clone https://github.com/coredns/deployment.git
cd deployment/kubernetes
#10.96.0.10 改为 service 网段的 第 10 个ip
./deploy.sh -s -i 10.96.0.10 | kubectl apply -f -
```



13、给机器打上role标签  

```shell
kubectl label node k8s-master1 node-role.kubernetes.io/master=''
kubectl label node k8s-master2 node-role.kubernetes.io/master=''
kubectl label node k8s-master3 node-role.kubernetes.io/master=''
kubectl taints node k8s-master1
```



14、集群验证  

验证 Pod 网络可访问性

​	同名称空间，不同名称空间可以使用  ip 互相访问

​	跨机器部署的 Pod 也可以互相访问



验证 Service 网络可访问性

​	集群机器使用 serviceIp 可以负载均衡访问

​	pod内部可以访问service域名 serviceName.namespace

​	pod可以访问跨名称空间的service



部署以下内容进行测试  

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-01
  namespace: default
  labels:
    app: nginx-01
spec:
  selector:
    matchLabels:
      app: nginx-01
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx-01
  spec:
    containers:
    - name: nginx-01
      image: nginx
      
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
  namespace: default
spec:
  selector:
    app: nginx-01
  type: ClusterIP
  ports:
  - name: nginx-svc
    port: 80
    targetPort: 80
    protocol: TCP

---
apiVersion: v1
kind: Namespace
metadata:
  name: hello
spec: {}

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-hello
  namespace: hello
  labels:
    app: nginx-hello
spec:
  selector:
    matchLabels:
      app: nginx-hello
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx-hello
    spec:
      containers:
      - name: nginx-hello
        image: nginx
    
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-svc-hello
  namespace: hello
spec:
  selector:
    app: nginx-hello
  type: ClusterIP
  ports:
  - name: nginx-svc-hello
    port: 80
    targetPort: 80
    protocol: TCP
```



给两个master标识为worker

```shell
kubectl label node k8s-node3 node-role.kubernetes.io/worker=''
kubectl label node k8s-master3 node-role.kubernetes.io/worker=''
kubectl label node k8s-node1 node-role.kubernetes.io/worker=''
kubectl label node k8s-node2 node-role.kubernetes.io/worker=''
```



给master1打上污点。二进制部署的集群，默认master是没有污点的，可以任意调度。我们最好给一个master打上污点，保证master最小可用  

```shell
kubectl label node k8s-master3 node-role.kubernetes.io/master=''
kubectl taint nodes k8s-master1 node-role.kubernetes.io/master=:NoSchedule
```



## 16 集群优化  

### 优化kubelet

更多参照： https://kubernetes.io/zh/docs/reference/config-api/kubelet-config.v1beta1/  

```shell
vi /etc/kubernetes/kubelet-conf.yml
```

kubeReserved： kubelet预留资源  

```yaml
kubeReserved:
  cpu: "500m"
  memory: 300m
  ephemeral-storage: 3Gi
systemReserved:
  cpu: "200m"
  memory: 500m
  ephemeral-storage: 3Gi
```

验证集群kube-proxy使用ipvs模式工作； 10249是每个节点 kube-proxy 的 metrics 信息端口，可以访问 /proxyMode 或者 /metrics 等

```shell
curl 127.0.0.1:10249/proxyMode
```



### 时区问题

很多应用镜像时区都是UTC，而不是本机时间（当然，前提是本机时间是对的，云服务器不存在这个问题）

我们不用每一个 Pod 都设置挂载本地时间

```shell
vi pod-preset-timezone.yaml
```

```yaml
apiVersion: settings.k8s.io/v1alpha1
kind: PodPreset
metadata:
  name: allow-localtime
  namespace: spinnaker
spec:
  selector:
    matchLabels:
  volumeMounts:
  - mountPath: /etc/localtime
    name: localtime
  volumes:
  - name: localtime
    hostPath:
      path: etc/localtime
```

```shell
kubectl apply -f pod-preset-timezone.yaml
```

docker hub下载来的几乎所有Pod都是UTC时间。

java -- Pod -- UTC

这个需要开启特性门控

```shell
vi /usr/lib/systemd/system/kube-apiserver.service
```

--runtime-config=settings.k8s.io/v1alpha1=true

添加 --enable-admission-plugins中加入 PodPreset

```shell
systemctl daemon-reload && systemctl restart kube-apiserver
```

可惜此特性在 1.20 以后废弃了。

PodPreset（Pod 预设置）的功能从 v1.11 版本开始出现，但是又在 v1.20 版本取消。

所以以后，使用Pod标准模板。挂载时区  

```yaml
volumeMounts:
- name: localtime
  mountPath: /etc/localtime
volumes:
- name: localtime
  hostPath:
    path: /usr/share/zoneinfo/Asia/Shanghai
```



